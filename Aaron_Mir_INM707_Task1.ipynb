{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# INM707 Coursework\n",
    "### Aaron Mir (Student Number: 160001207)\n",
    "<img src=\"All_Tasks.png\" alt=\"All_TasksOverview\" width=\"700\"/>  <img src=\"Task_1.png\" alt=\"Task_1\" width=\"700\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##--------------------Coding References---------------------##\n",
    "# Percentage of borrowed code: X% - \n",
    "# [1] "
   ]
  },
  {
   "source": [
    "Environment: The agent is preparing for an upcoming mission in which they are to infiltrate an enemy stronghold to gather intelligence on a potential coup d'Ã©tat. The enemy land is in the form of an NxN grid with each grid containing one stronghold of size N/2 x N or N x N/2 (starting on either side of the middle column (randomly decided)) and a wide river surrounding the entire enemy land. The stronghold has as many entrances from the mainland as (1/4)N and contains dangerous enemy combatants (at random positions) within the stronghold that move around per step (stochasticity). The land inside the stronghold has no transition probability associated with it (deterministic). The land surrounding the stronghold (the shore) is booby-trapped and covered in mist which means that there is both land which can kill/hurt the agent as well as normal land and a transition probability associated with each step (stochasticity) outside the stronghold. The amount of shore that has traps on it is (1/4)N x rows of shore. The row/column of values immediately beside the stronghold is normal land. The goal of the agent is to infiltrate the stronghold and gather the intelligence without being seen/killed by enemy combatants or booby traps.\n",
    "\n",
    "The agent starts in a random cell which belongs to the land surrounding the stronghold.\n",
    "\n",
    "State of the agent: Governed by the index of the cell it is on.\n",
    "Set of states of the environment: Governed by the index of the agent and the index of moving enemies.\n",
    "\n",
    "Rewards: +10 for getting intelligence, -100 for getting hurt by a combatant or booby trap, -1 for moving into a wall or water\n",
    "\n",
    "Terminal States: agent moves to the intelligence, if agent moves to a trap and dies, if the agent moves to an enemy combatant, if agent runs out of time\n",
    "\n",
    "_ represents normal land [0], X represents a river/wall [1], A represents the agent [2], T represents a booby trap [3], E represents an enemy [4], I represents the intelligence [5]\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Checklist:\n",
    "    \n",
    "    1. Estabilish states (done, represented by the coordinates of the agent on the grid)\n",
    "    2. Estabilish transitions matrix.\n",
    "    3. Estabilish transition probability matrix. - deterministic right now\n",
    "    3. Estabilish rewards matrix.\n",
    "    4. Establish return G.\n",
    "    5. Sort out enemy movement.\n",
    "    5. Improve visualisation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do:\n",
    "class Stronghold():\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.actions = {0: 'Up', 1: 'Down', 2: 'Left', 3: 'Right'}\n",
    "        self.land = self.env_gen()\n",
    "        self.position_agent = None                                 # initial position of the agent will be decided by resetting the environment\n",
    "        self.time_elapsed = 0                                      # run time\n",
    "        self.time_limit = self.size**2\n",
    "        self.R = self.fill_reward_matrix()\n",
    "        self.P = self.fill_transition_probability_matrix()\n",
    "        self.dict_map_display={ 0:'_',  # normal land\n",
    "                                1:'X',  # river/wall\n",
    "                                2:'A',  # agent\n",
    "                                3:'T',  # trap\n",
    "                                4:'E',  # enemy\n",
    "                                5:'I'}  # intelligence\n",
    "                    \n",
    "    def env_gen(self):\n",
    "        land = np.zeros((self.size, self.size))\n",
    "        land[0,:] = 1                                               # establish the river\n",
    "        land[:,0] = 1\n",
    "        land[self.size-1,:] = 1\n",
    "        land[:,self.size-1] = 1\n",
    "        self.column_choice = np.random.choice((self.size//2-1, self.size//2+1)) # random choice whether stronghold starts from the left or right of the 'central' column\n",
    "        land[1:self.size-1, self.column_choice] = 1\n",
    "        if self.column_choice == self.size//2-1:                              # if stronghold is on left\n",
    "            land[1, 0:self.column_choice] = 1                                # establish the walls of stronghold\n",
    "            land[self.size-2, 0:self.column_choice] = 1\n",
    "            land[1:self.size-1, 0] = 1                                   \n",
    "            for col in land[1:self.size-1, self.column_choice+2:self.size-1].T:\n",
    "                traps = []\n",
    "                for i in range(int(np.round(1/4*len(col)))):        # make as many traps as 1/4 of the length of each column in the shore\n",
    "                    trap = np.random.choice(np.setdiff1d(range(len(col)), traps))\n",
    "                    col[trap] = 3\n",
    "                    traps.append(trap)\n",
    "            for col in land[2:self.size-2, 1:self.column_choice].T:\n",
    "                enemies = []\n",
    "                for i in range(int(np.round(1/4*len(col)))):        # populate enemies randomly inside the stronghold\n",
    "                    enemy = np.random.choice(np.setdiff1d(range(len(col)), enemies))\n",
    "                    col[enemy] = 4\n",
    "                    enemies.append(enemy)\n",
    "            intel_row = np.random.randint(1, len(land[2:self.size-2, 1:self.column_choice-1]))\n",
    "            intel_col = np.random.randint(1, len(land[2:self.size-2, 1:self.column_choice-1].T))\n",
    "            land[intel_row+2][self.column_choice-intel_col] = 5            # randomly insert intelligence into stronghold \n",
    "        else:                                                              # if stronghold is on right                          \n",
    "            land[1, self.column_choice:self.size] = 1\n",
    "            land[self.size-2, self.column_choice:self.size] = 1\n",
    "            land[1:self.size-1, self.size-1] = 1\n",
    "            for col in land[1:self.size-1, 1:self.column_choice-1].T:\n",
    "                traps = []\n",
    "                for i in range(int(np.round(1/4*len(col)))):        # make as many traps as 1/4 of the length of each column in the shore\n",
    "                    trap = np.random.choice(np.setdiff1d(range(len(col)), traps))\n",
    "                    col[trap] = 3\n",
    "                    traps.append(trap)\n",
    "            for col in land[2:self.size-2, self.column_choice+1:self.size-1].T:\n",
    "                enemies = []\n",
    "                for i in range(int(np.round(1/4*len(col)))):        # populate enemies randomly inside the stronhold\n",
    "                    enemy = np.random.choice(np.setdiff1d(range(len(col)), enemies))\n",
    "                    col[enemy] = 4\n",
    "                    enemies.append(enemy)\n",
    "            intel_row = np.random.randint(1, len(land[2:self.size-2, self.column_choice+1:self.size-1]))              \n",
    "            intel_col = np.random.randint(1, len(land[2:self.size-2, self.column_choice+1:self.size-1].T))\n",
    "            land[intel_row+2][self.column_choice+intel_col] = 5            # randomly insert intelligence into stronghold\n",
    "        entrances = []\n",
    "        for i in range(int(np.round(1/4*len(land[2:self.size-2, self.column_choice])))):        # make as many entrances as 1/4 of the length of the front wall \n",
    "            entrance = np.random.choice(np.setdiff1d(range(len(land[2:self.size-2, self.column_choice])), entrances))\n",
    "            land[2:self.size-2, self.column_choice][entrance] = 0\n",
    "            entrances.append(entrance)\n",
    "        return land\n",
    "        \n",
    "    def get_empty_cells_shore(self, n_cells):\n",
    "        if self.column_choice == self.size//2-1:\n",
    "            empty_cells_coord = np.where(self.land[1:self.size-1, self.column_choice+1:self.size-1] == 0)\n",
    "            selected_indices = np.random.choice(np.arange(len(empty_cells_coord[0])), n_cells)\n",
    "            selected_coordinates = empty_cells_coord[0][selected_indices]+1, empty_cells_coord[1][selected_indices]+len(self.land[1][:self.column_choice+1])\n",
    "        if self.column_choice == self.size//2+1:\n",
    "            empty_cells_coord = np.where(self.land[1:self.size-1, 1:self.column_choice] == 0)\n",
    "            selected_indices = np.random.choice(np.arange(len(empty_cells_coord[0])), n_cells)\n",
    "            selected_coordinates = empty_cells_coord[0][selected_indices]+1, empty_cells_coord[1][selected_indices]\n",
    "        if n_cells == 1:\n",
    "            return np.asarray(selected_coordinates).reshape(2,)\n",
    "        return selected_coordinates\n",
    "\n",
    "    def step(self, action):\n",
    "        # agent moves\n",
    "        current_position = np.array((self.position_agent[0], self.position_agent[1])) # saving the current position in case agent hits a wall\n",
    "        reward_step = 0\n",
    "        if action == 'up':                                          # action is 'up', 'down', 'left', or 'right'\n",
    "            self.position_agent[0] -= 1   \n",
    "        if action == 'down':\n",
    "            self.position_agent[0] += 1 \n",
    "        if action == 'left':\n",
    "            self.position_agent[1] -= 1\n",
    "        if action == 'right':\n",
    "            self.position_agent[1] += 1\n",
    "\n",
    "        # enemies move randomly - np.where 4 do random actions for, if they hit a wall or go outside the stronghold then bring them back in\n",
    "        enemy_move = np.random.choice(self.actions)\n",
    "\n",
    "        # calculate total reward\n",
    "        if self.land[self.position_agent[0]][self.position_agent[1]] == 1:\n",
    "            reward_step -= 1\n",
    "            self.position_agent = current_position                                                         \n",
    "        if self.land[self.position_agent[0]][self.position_agent[1]] == 3 or self.land[self.position_agent[0]][self.position_agent[1]] == 4:\n",
    "            reward_step -= 100\n",
    "        if self.land[self.position_agent[0]][self.position_agent[1]] == 5:\n",
    "            reward_step += 10\n",
    "            done = True                     # termination condition\n",
    "\n",
    "        # calculate observations\n",
    "        observations = self.observe()\n",
    "        \n",
    "        # time-limit termination condition\n",
    "        if self.time_elapsed == self.time_limit:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            self.time_elapsed += 1              # update time\n",
    "            reward_step -= 1                    # negative reward per time-step\n",
    "        \n",
    "        new_state = self.state_transition_matrix[self.position_agent[0]][self.position_agent[1]][self.dict_actions[action]]\n",
    "        return new_state, reward, done\n",
    "    \n",
    "    def reset(self):\n",
    "        self.time_elapsed = 0                                                 # put time_elapsed to 0\n",
    "        self.position_agent = np.asarray(self.get_empty_cells_shore(1))       # position of the agent is a random cell on the shore numpy array\n",
    "        \n",
    "        # Calculate observations\n",
    "        #observations = self.calculate_observations()\n",
    "        #return observations\n",
    "\n",
    "    def render(self):                                                       # displays the land\n",
    "        envir_with_agent = self.land.copy()\n",
    "        envir_with_agent[self.position_agent[0], self.position_agent[1]] = 2\n",
    "        full_repr = \"\"\n",
    "        for r in range(self.size):\n",
    "            line = \"\"\n",
    "            for c in range(self.size):\n",
    "                string_repr = self.dict_map_display[envir_with_agent[r,c]]    \n",
    "                line += \"{0:2}\".format(string_repr)\n",
    "            full_repr += line + \"\\n\"\n",
    "        print(full_repr)\n",
    "\n",
    "    def fill_transition_probability_matrix(self):\n",
    "        state_transition_matrix = []\n",
    "        for i in range(len(self.land)):\n",
    "            for j in range(len(self.land)):\n",
    "                if i-1 < 0:\n",
    "                    state_up = None\n",
    "                else: state_up = np.asarray([i-1, j])\n",
    "                if i+1 > len(self.land)-1:\n",
    "                    state_down = None\n",
    "                else: state_down = np.asarray([i+1, j])\n",
    "                if j-1 < 0:\n",
    "                    state_left = None\n",
    "                else: state_left = np.asarray([i, j-1])\n",
    "                if j+1 > len(self.land)-1:\n",
    "                    state_right = None\n",
    "                else: state_right = np.asarray([i, j+1])\n",
    "                state_transition_matrix.append([state_up, state_down, state_left, state_right])\n",
    "        state_transition_array = np.array(state_transition_matrix).reshape(self.size*self.size, 4, 1) \n",
    "        P = np.vsplit(state_transition_array, self.size) # P maps the position of the agent (the state) and action to reachable states - N*N is number of reachable states, 4 is number of possible actions \n",
    "        return P\n",
    "        \n",
    "    def fill_reward_matrix(self):\n",
    "        reward_matrix = []\n",
    "        for i in range(len(self.land)):\n",
    "            for j in range(len(self.land)):\n",
    "                if i-1 < 0:\n",
    "                    reward_up = None\n",
    "                else: reward_up = self.land[i-1][j]\n",
    "                if i+1 > len(self.land)-1:\n",
    "                    reward_down = None\n",
    "                else: reward_down = self.land[i+1][j]\n",
    "                if j-1 < 0:\n",
    "                    reward_left = None\n",
    "                else: reward_left = self.land[i][j-1]\n",
    "                if j+1 > len(self.land)-1:\n",
    "                    reward_right = None\n",
    "                else: reward_right = self.land[i][j+1]\n",
    "                reward_matrix.append([reward_up, reward_down, reward_left, reward_right])\n",
    "        reward_array = np.array(reward_matrix).reshape(self.size*self.size, 4, 1) \n",
    "        R = np.vsplit(reward_array, self.size) # R maps the position of the agent (the state) and action to rewards\n",
    "        return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the state of the agent is entirely described by either the coordinates of the cell it is on, or the index of the cell.\n",
    "# the value function (can be represented as a dictionary, or an array) that maps the state to the value of the state. As we don't know the true value at the beginning, it will be initialized at 1.\n",
    "# the current state is given by self.position_agent gives the state i.e. its index\n",
    "# the actions will be the direct cells that an agent can go to from a particular cell or up down left right\n",
    "# the rewards will be given to a robot if a cell/state is directly reachable from the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X X X X X X X X X X X X X X X X X X X X X \nX T _ _ T _ _ _ _ _ _ X X X X X X X X X X \nX _ _ T T T _ T _ T _ X E E _ _ E _ _ E X \nX _ T _ _ _ T _ T _ _ X _ _ _ _ _ _ _ E X \nX _ _ T T _ T _ _ _ _ X E _ E _ _ _ _ _ X \nX T T T _ _ _ _ T _ _ X _ _ _ E _ E _ _ X \nX _ _ _ _ T _ _ A _ _ X _ E _ _ _ _ _ E X \nX _ _ _ _ _ _ _ _ _ _ _ E _ _ _ E _ _ _ X \nX _ _ _ _ _ T _ _ T _ X _ E E E _ _ _ _ X \nX _ T _ _ _ _ _ _ _ _ X _ _ _ E _ _ E E X \nX _ _ _ T T T T _ _ _ _ _ _ _ _ _ _ _ _ X \nX _ _ T T T _ T T T _ X _ _ _ _ _ E _ _ X \nX _ _ _ _ _ _ _ _ T _ X _ E _ _ _ E _ _ X \nX _ T _ _ _ _ _ T _ _ _ E _ _ _ _ I E _ X \nX _ _ T _ T _ _ _ _ _ X _ _ _ _ _ _ E _ X \nX T _ _ _ _ _ T _ T _ X _ _ _ _ E _ _ _ X \nX T _ _ _ _ T T _ _ _ X _ _ _ E _ _ _ _ X \nX T _ _ _ _ _ _ _ _ _ _ _ _ E _ E _ E _ X \nX _ T _ _ _ _ _ _ _ _ X _ _ E _ _ _ _ _ X \nX _ _ _ _ _ _ _ T _ _ X X X X X X X X X X \nX X X X X X X X X X X X X X X X X X X X X \n\n"
     ]
    }
   ],
   "source": [
    "stronghold = Stronghold(21)\n",
    "stronghold.reset()\n",
    "stronghold.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([6, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "stronghold.position_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[array([8, 4])]\n [array([10,  4])]\n [array([9, 3])]\n [array([9, 5])]]\n"
     ]
    }
   ],
   "source": [
    "print(stronghold.P[9][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[3.0],\n",
       "       [0.0],\n",
       "       [0.0],\n",
       "       [0.0]], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "stronghold.R[9][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policies():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def random_policy(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}