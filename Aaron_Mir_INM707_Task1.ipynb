{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM707 Coursework Task 1\r\n",
    "### Aaron Mir (Student Number: 160001207)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up MDP Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##--------------------Coding References---------------------## <img src=\"All_Tasks.png\" alt=\"All_TasksOverview\" width=\"700\"/>  <img src=\"Task_1.png\" alt=\"Task_1\" width=\"700\"/>\r\n",
    "# Percentage of borrowed code: 5% - Modelled environment generaly like OpenAI Discrete Environments\r\n",
    "# [1] OpenAI (2020) OpenAI Discrete Environments, \r\n",
    "# Available at: https://github.com/openai/gym/tree/38a1f630dc9815a567aaf299ae5844c8f8b9a6fa/gym/envs/toy_text (Accessed: 28th Feb 2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\r\n",
    "## Stronghold\r\n",
    "\r\n",
    "The robot agent is preparing for an upcoming mission in which they are to infiltrate an enemy stronghold to gather intelligence on a potential coup d'Ã©tat. The enemy land is in the form of an NxN grid with each grid containing one stronghold of size N x N/2 (starting on either side of the middle column (randomly decided)) and a wide river surrounding the entire enemy land. The entire land is very misty, therefore the robot may not step where it is supposed to. Each move has a transition probability associated with it (0.7 to move to the chosen state or 0.3 to move to one of the other sides) (stochasticity). The stronghold has as many entrances from the mainland as (1/4)N and contains (1/5)N dangerous enemy combatants (at random positions) within the stronghold that move around randomly per step (stochasticity).  The amount of shore that has traps on it is (1/4)N x rows of shore. The column immediately beside the stronghold is normal land to prevent entrance blocking. The goal of the agent is to infiltrate the stronghold and gather the intelligence without being killed by enemy combatants or booby traps as fast as possible.\r\n",
    "\r\n",
    "This is a type of gridworld environment. The size (number of states) can be controlled by adjusting the grid dimensions.\r\n",
    "\r\n",
    "The environment is defined as follows:\r\n",
    "\r\n",
    "- The environment is a rectangular grid of states/cells. There are five different types of cells as indicated by the following cell labels: \r\n",
    "\r\n",
    "    - _ labels cells that are safe to step on i.e. normal land [0]\r\n",
    "\r\n",
    "    - X labels the cells that are walls i.e. river or wall and if the agent enters a wall cell, there is a penalty of -1 [1]\r\n",
    "    \r\n",
    "    - A labels the cell the agent is on, starts on a random cell on the shore [2]\r\n",
    "    \r\n",
    "    - T labels booby-trap cells and if the agent enters a booby-trap cell there is a pentalty of -1000 and the episode ends [3]\r\n",
    "\r\n",
    "    - E labels enemy cells and if the agent enters an enemy cell there is a pentalty of -1000 and the episode ends [4]\r\n",
    "\r\n",
    "    - I labels the intelligence cell and when reached gives a reward of 1000 and the episode ends [5]\r\n",
    "\r\n",
    "- There are four possible actions (Up, Down, Left, Right). \r\n",
    "\r\n",
    "- The transition function moves the agent in the expected direction with 0.7 probability, and there is a 0.3 probability of transitioning to one of the sides.\r\n",
    "\r\n",
    "- There is a reward of -1 for each action taken by the agent, which is intended to encourage the agent to reach the goal as fast as possible. If the agent runs out of time, the episode ends. \r\n",
    "\r\n",
    "- Episodes end whenever the agent falls in a booby-trap, gets killed by an enemy, reaches the goal/intelligence or runs out of time. The end-of-episode is modelled by transitioning to a zero-reward terminal state (all actions lead to that state). \r\n",
    "\r\n",
    "State of the agent: Governed by the state number of the cell it is on.\r\n",
    "\r\n",
    "States of the environment: Governed by the state number of the agent and the state number of moving enemies.\r\n",
    "\r\n",
    "Number of states of the environment is given by the size of the grid and the size of the stronghold in which enemies can move. e.g. for a 10x10 grid with a stronghold size of 6x3 and 6 enemies, the number of states is 100 - 6x3-1 +  (6x3-1 (because of starting state)/6)  - 1 (because of the intelligence in the stronghold)\r\n",
    "\r\n",
    "Rewards/Penalties: +1000 for getting intelligence, -1000 for getting hurt by a combatant or booby trap, -1 for moving into a wall or water, -1 per transition because of fuel constraints\r\n",
    "\r\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations will be fed into the dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist:\n",
    "\n",
    "The MDP consists of states, a transition probability, a reward function, and also actions\n",
    "\n",
    "    1. Estabilish states (done, represented by the cell the agent is on)\n",
    "    2. Estabilish transition probability matrix (done)\n",
    "    3. Estabilish rewards matrix (done)\n",
    "    4. Establish return G (Not really needed?)\n",
    "    5. Sort out enemy movement (done, enemies move within the stronghold)\n",
    "    6. Implement partial observability (done)\n",
    "    7. Improve visualisation (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import sys\r\n",
    "import time\r\n",
    "from IPython.display import clear_output\r\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=sys.maxsize, precision = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stronghold(): \r\n",
    "    def __init__(self, size, simplicity):\r\n",
    "        self.size = size\r\n",
    "        self.actions = {0: 'Up', 1: 'Down', 2: 'Left', 3: 'Right'}\r\n",
    "        self.land = self.env_gen(simplicity)\r\n",
    "        self.position_agent = None                                 # initial position of the agent will be decided by resetting the environment\r\n",
    "        self.time_elapsed = 0                                      # run time\r\n",
    "        self.simplicity = simplicity\r\n",
    "        self.time_limit = self.size**2\r\n",
    "        self.numActions, self.numStates = len(self.actions), size * size\r\n",
    "        self.prob_of_tripping = [0.7, 0.1, 0.1, 0.1]               # 0.7 probability of going to where it chooses and 0.3 to go to differently\r\n",
    "        self.P = {s:{a:[] for a in range(self.numActions)} for s in range(self.numStates)}      # dictionary mapping states and actions\r\n",
    "        self.transitionProb = np.zeros((self.numActions, self.numStates + 1, self.numStates + 1))\r\n",
    "        self.transitionReward = np.zeros((self.numStates + 1, self.numActions))\r\n",
    "        self.fill_transitions_probability()\r\n",
    "        self.dict_map_display={ 0:'_',          # normal land\r\n",
    "                                1:'X',          # river/wall\r\n",
    "                                2:'A',          # agent\r\n",
    "                                3:'T',          # trap\r\n",
    "                                4:'E',          # enemy\r\n",
    "                                5:'I'}          # intelligence\r\n",
    "                    \r\n",
    "    def env_gen(self, simplicity):\r\n",
    "        land = np.zeros((self.size, self.size))\r\n",
    "        land[0,:] = 1                                               # establish the river\r\n",
    "        land[:,0] = 1\r\n",
    "        land[self.size-1,:] = 1\r\n",
    "        land[:,self.size-1] = 1\r\n",
    "        self.column_choice = np.random.choice((self.size//2-1, self.size//2+1)) # random choice whether stronghold starts from the left or right of the 'central' column\r\n",
    "        land[1:self.size-1, self.column_choice] = 1\r\n",
    "        if self.column_choice == self.size//2-1:                              # if stronghold is on left\r\n",
    "            land[1:self.size-1, 0] = 1                                        # establish the walls of stronghold\r\n",
    "            for col in land[1:self.size-1, self.column_choice+2:self.size-1].T:\r\n",
    "                traps = []\r\n",
    "                for i in range(int(np.round(1/simplicity*len(col)))):        # make as many traps as 1/simplicity of the length of each column in the shore\r\n",
    "                    trap = np.random.choice(np.setdiff1d(range(len(col)), traps))\r\n",
    "                    col[trap] = 3\r\n",
    "                    traps.append(trap)\r\n",
    "            for col in land[1:self.size-1, 1:self.column_choice].T:\r\n",
    "                enemies = []\r\n",
    "                for i in range(int(np.round(1/simplicity*len(col)))):        # populate enemies randomly inside the stronghold\r\n",
    "                    enemy = np.random.choice(np.setdiff1d(range(len(col)), enemies))\r\n",
    "                    col[enemy] = 4\r\n",
    "                    enemies.append(enemy)\r\n",
    "            intel_row = np.random.randint(1, len(land[1:self.size-1]))\r\n",
    "            intel_col = np.random.randint(1, len(land[1:self.column_choice-1]))\r\n",
    "            land[intel_row+1, intel_col] = 5\r\n",
    "            self.position_intel = [intel_row+1, intel_col]            # randomly insert intelligence into stronghold\r\n",
    "        else:                                                              # if stronghold is on right                          \r\n",
    "            land[1:self.size-1, self.size-1] = 1                            # establish the walls of stronghold\r\n",
    "            for col in land[1:self.size-1, 1:self.column_choice-1].T:\r\n",
    "                traps = []\r\n",
    "                for i in range(int(np.round(1/simplicity*len(col)))):        # make as many traps as 1/simplicity of the length of each column in the shore\r\n",
    "                    trap = np.random.choice(np.setdiff1d(range(len(col)), traps))\r\n",
    "                    col[trap] = 3\r\n",
    "                    traps.append(trap)\r\n",
    "            for col in land[1:self.size-1, self.column_choice+1:self.size-1].T:\r\n",
    "                enemies = []\r\n",
    "                for i in range(int(np.round(1/simplicity*len(col)))):        # populate enemies randomly inside the stronghold\r\n",
    "                    enemy = np.random.choice(np.setdiff1d(range(len(col)), enemies))\r\n",
    "                    col[enemy] = 4\r\n",
    "                    enemies.append(enemy)\r\n",
    "            intel_row = np.random.randint(1, len(land[1:self.size-1]))              \r\n",
    "            intel_col = np.random.randint(1, len(land[self.column_choice+1:self.size-1]))\r\n",
    "            land[intel_row+1, self.column_choice+intel_col] = 5\r\n",
    "            self.position_intel = [intel_row+1, self.column_choice+intel_col]             # randomly insert intelligence into stronghold\r\n",
    "        entrances = []\r\n",
    "        for i in range(int(np.round((1/2)*len(land[2:self.size-2, self.column_choice])))):        # make as many entrances as 1/2 of the length of the front wall \r\n",
    "            entrance = np.random.choice(np.setdiff1d(range(len(land[2:self.size-2, self.column_choice])), entrances))\r\n",
    "            land[2:self.size-2, self.column_choice][entrance] = 0\r\n",
    "            entrances.append(entrance)\r\n",
    "        return land\r\n",
    "\r\n",
    "    def to_state(self, row, col):\r\n",
    "            return row*self.size + col\r\n",
    "\r\n",
    "    def inc(self, row, col, a):\r\n",
    "        if a == 0:\r\n",
    "            row = max(row-1,0)\r\n",
    "        elif a == 1:\r\n",
    "            row = min(row+1, self.size-1)       \r\n",
    "        elif a == 2:\r\n",
    "            col = max(col-1,0)\r\n",
    "        elif a == 3:\r\n",
    "            col = min(col+1,self.size-1)\r\n",
    "        return (row, col)\r\n",
    "    \r\n",
    "    def fill_transitions_probability(self):\r\n",
    "        for row in range(self.size):\r\n",
    "            for col in range(self.size):\r\n",
    "                s = self.to_state(row, col)\r\n",
    "                for a in range(4):\r\n",
    "                    trans_prob = self.P[s][a]\r\n",
    "                    number = self.land[row, col]\r\n",
    "                    if number == 3: # trap\r\n",
    "                        trans_prob.append((1.0, s, 0, True))\r\n",
    "                        self.transitionProb[a, s, self.numStates] = 1.0\r\n",
    "                        self.transitionReward[s, a] = -1000\r\n",
    "                    elif number == 4: # enemy\r\n",
    "                        trans_prob.append((1.0, s, 0, True))\r\n",
    "                        self.transitionProb[a, s, self.numStates] = 1.0\r\n",
    "                        self.transitionReward[s, a] = -1000\r\n",
    "                    elif number == 5: # intelligence\r\n",
    "                        trans_prob.append((1.0, s, 0, True))\r\n",
    "                        self.transitionProb[a, s, self.numStates] = 1.0\r\n",
    "                        self.transitionReward[s, a] = 1000\r\n",
    "                    else:\r\n",
    "                        for b, p in zip([a, (a+1)%4, (a+2)%4, (a+3)%4], self.prob_of_tripping):\r\n",
    "                            newrow, newcol = self.inc(row, col, b)\r\n",
    "                            newstate = self.to_state(newrow, newcol)\r\n",
    "                            newnumber = self.land[newrow][newcol]\r\n",
    "                            done = False\r\n",
    "                            if newnumber == 3:                  # if enters trap\r\n",
    "                                rew = -1000\r\n",
    "                            elif newnumber == 4:                # if enemy kills\r\n",
    "                                rew = -1000\r\n",
    "                            elif newnumber == 5:                # if intelligence is caught\r\n",
    "                                rew = 1000\r\n",
    "                            else:\r\n",
    "                                rew = -1                        # penalty for time-step               \r\n",
    "                            trans_prob.append((p, newstate, rew, done))\r\n",
    "                            self.transitionProb[a, s, newstate] += p\r\n",
    "                            self.transitionReward[s, a] = -1 \r\n",
    "    \r\n",
    "    def getSuccessors(self, s, a): \r\n",
    "        # Take a state and an action as input, and return a list of pairs, where each pair (s', p) is a successor state s' with non-zero probability and p is the probability of transitioning to p.\r\n",
    "        idx = [0, 3, 1, 2] # up left right down rearrange to up down left right [0, 3, 1, 2]    left right up down [1, 2, 0, 3]\r\n",
    "        next_states = np.nonzero(self.transitionProb[a, s, :]) # np.nonzero(self.transitionProb[a, s, :])\r\n",
    "        probs = self.transitionProb[a, s, next_states]\r\n",
    "        if np.size(next_states[0]) == 1:\r\n",
    "            return next_states[0]\r\n",
    "        else:        \r\n",
    "            return [(s,p) for s,p in zip(next_states[0][idx], probs[0][idx])]  \r\n",
    "\r\n",
    "    def gettransitionProb(self, s, a, ns):\r\n",
    "        # Take a state, an action, a next state as input, and return the probability of the transition \r\n",
    "        return self.transitionProb[a, s, ns]\r\n",
    "\r\n",
    "    def getReward(self, s, a):\r\n",
    "        # Take a state and an action as input, and return the reward of that.\r\n",
    "        return self.transitionReward[s, a]\r\n",
    "\r\n",
    "    def getStateSpace(self):\r\n",
    "        return self.transitionProb.shape[1]\r\n",
    "    \r\n",
    "    def getActionSpace(self):\r\n",
    "        return self.transitionProb.shape[0]\r\n",
    "\r\n",
    "    def normal_shore(self, n_cells):\r\n",
    "        if self.column_choice == self.size//2-1:\r\n",
    "            empty_cells_coord = np.where(self.land[1:self.size-1, self.column_choice+1:self.size-1] == 0)\r\n",
    "            selected_indices = np.random.choice(np.arange(len(empty_cells_coord[0])), n_cells)\r\n",
    "            selected_coordinates = empty_cells_coord[0][selected_indices]+1, empty_cells_coord[1][selected_indices]+len(self.land[1][:self.column_choice+1])\r\n",
    "        if self.column_choice == self.size//2+1:\r\n",
    "            empty_cells_coord = np.where(self.land[1:self.size-1, 1:self.column_choice] == 0)\r\n",
    "            selected_indices = np.random.choice(np.arange(len(empty_cells_coord[0])), n_cells)\r\n",
    "            selected_coordinates = empty_cells_coord[0][selected_indices]+1, empty_cells_coord[1][selected_indices]+1\r\n",
    "        if n_cells == 1:\r\n",
    "            return np.asarray(selected_coordinates).reshape(2,)\r\n",
    "        return selected_coordinates\r\n",
    "\r\n",
    "    def step(self, action): # action is 0, 1, 2, 3 for up, down, left, right\r\n",
    "        for i, j in zip(*np.where(self.land == 4)):     # enemies move randomly - they do not move if their choice is a wall, the intelligence, another enemy or the stronghold entrance column\r\n",
    "            move = np.random.choice(('up', 'down', 'left', 'right'))\r\n",
    "            if move == 'up' and self.land[i-1, j] != 1 and self.land[i-1, j] != 4 \\\r\n",
    "                and self.land[i-1, j] != 5:\r\n",
    "                self.land[i, j] = 0\r\n",
    "                self.land[i-1, j] = 4\r\n",
    "            if move == 'down' and self.land[i+1, j] != 1 and self.land[i+1, j] != 4 \\\r\n",
    "                and self.land[i+1, j] != 5:\r\n",
    "                self.land[i, j] = 0\r\n",
    "                self.land[i+1][j] = 4\r\n",
    "            if move == 'left' and self.land[i, j-1] != 1 and self.land[i, j-1] != 4 \\\r\n",
    "                and self.land[i, j-1] != 5 and j-1 != self.column_choice:\r\n",
    "                self.land[i, j] = 0\r\n",
    "                self.land[i, j-1] = 4\r\n",
    "            if move == 'right' and self.land[i, j+1] != 1 and self.land[i, j+1] != 4 \\\r\n",
    "                and self.land[i, j+1] != 5 and j+1 != self.column_choice:\r\n",
    "                self.land[i, j] = 0\r\n",
    "                self.land[i, j+1] = 4\r\n",
    "\r\n",
    "        current_position = np.array((self.position_agent[0], self.position_agent[1])) # saving the current position and state in case agent hits a wall\r\n",
    "        current_state = self.agent_state\r\n",
    "        options = self.getSuccessors(current_state, action)                           # chosen action gets 0.7 prob \r\n",
    "        if np.size(options) != 1:\r\n",
    "            probs = [i[1] for i in options]                                               #  list of probabilities\r\n",
    "            choice = np.random.choice((0, 1, 2, 3), p=probs)                              #  make a probability choice\r\n",
    "            state_choice = options[choice][0]\r\n",
    "            ind = np.where(self.P[self.agent_state][action][:][:] == state_choice)[0][0]\r\n",
    "            prob, new_state, reward, done = self.P[self.agent_state][action][ind]    # new prob, state, reward, done for the choice                 \r\n",
    "\r\n",
    "            self.agent_state = new_state                                                # agent state and cell index position gets updated\r\n",
    "            if choice == 0:                                                             # up                                   \r\n",
    "                self.position_agent[0] -= 1\r\n",
    "            if choice == 1:                                                             # down\r\n",
    "                self.position_agent[0] += 1\r\n",
    "            if choice == 2:                                                             # left\r\n",
    "                self.position_agent[1] -= 1 \r\n",
    "            if choice == 3:                                                             # right\r\n",
    "                self.position_agent[1] += 1\r\n",
    "        \r\n",
    "            if self.land[self.position_agent[0], self.position_agent[1]] == 1:          # condition where they walk into a wall\r\n",
    "                self.position_agent = current_position                                  # state and position of agent = old state and position\r\n",
    "                new_state = current_state\r\n",
    "                self.agent_state = new_state\r\n",
    "                reward -= 1                                                             # additional penalty for bumping into wall\r\n",
    "        else:\r\n",
    "            prob, new_state, reward, done = self.P[self.agent_state][action][0]\r\n",
    "            if self.time_elapsed == self.time_limit:                                    # time-limit termination condition\r\n",
    "                done = True\r\n",
    "                print ('Time ran out')\r\n",
    "            else:\r\n",
    "                self.time_elapsed += 1                                                  # update time\r\n",
    "                    \r\n",
    "        return (int(new_state), reward, done, {\"prob\" : prob})                      # return state, reward, done, info\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.time_elapsed = 0                                              # put time_elapsed to 0\r\n",
    "        self.position_agent = np.asarray(self.normal_shore(1))             # position of the agent is a random cell on the shore\r\n",
    "        self.agent_state = self.to_state(self.position_agent[0], self.position_agent[1])\r\n",
    "        self.starting_pos = self.position_agent               \r\n",
    "        return self.agent_state                                             # return current state as observation\r\n",
    "\r\n",
    "    def render(self):                                                       # display\r\n",
    "        envir_with_agent = self.land.copy()\r\n",
    "        envir_with_agent[self.position_agent[0], self.position_agent[1]] = 2\r\n",
    "        \r\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\r\n",
    "        im = ax.imshow(envir_with_agent) \r\n",
    "\r\n",
    "        ax.set_xticks(np.arange(len(envir_with_agent)))\r\n",
    "        ax.set_yticks(np.arange(len(envir_with_agent)))\r\n",
    "        \r\n",
    "        for i in range(len(envir_with_agent)):                              # loop over data dimensions and create text annotations\r\n",
    "            for j in range(len(envir_with_agent)):\r\n",
    "                if envir_with_agent[i, j] == 0:\r\n",
    "                    caption = \"\"\r\n",
    "                elif envir_with_agent[i, j] == 1:\r\n",
    "                    caption = \"X\"\r\n",
    "                elif envir_with_agent[i, j] == 2:\r\n",
    "                    caption = \"A\"\r\n",
    "                elif envir_with_agent[i, j] == 3:\r\n",
    "                    caption = \"T\"\r\n",
    "                elif envir_with_agent[i, j] == 4:\r\n",
    "                    caption = \"E\"\r\n",
    "                elif envir_with_agent[i, j] == 5:\r\n",
    "                    caption = \"I\"\r\n",
    "                else:\r\n",
    "                    caption = \"\"    \r\n",
    "                text = ax.text(j, i, caption, ha=\"center\", va=\"center\", color=\"r\", fontsize='xx-large')\r\n",
    "\r\n",
    "        ax.set_title(\"Stronghold\", fontsize=15) # 35 for 10x10 \r\n",
    "        fig.tight_layout()\r\n",
    "        plt.axis('off')\r\n",
    "        plt.show()\r\n",
    "    \r\n",
    "    \r\n",
    "    # def plot_transition_matrix(self):\r\n",
    "    #     fig, ax = plt.subplots()\r\n",
    "    #     im = ax.imshow(self.transitionProb, cmap ='Set3') \r\n",
    "    #     plt.show()\r\n",
    "\r\n",
    "    # def plot_reward_matrix(self):\r\n",
    "    #     fig, ax = plt.subplots()\r\n",
    "    #     im = ax.imshow(self.transitionReward, cmap ='Set3') \r\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 360x360 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_size = 11\r\n",
    "simplicity = 4\r\n",
    "stronghold = Stronghold(env_size, simplicity)\r\n",
    "stronghold.reset()\r\n",
    "stronghold.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(): \r\n",
    "    return np.random.randint(0, stronghold.getActionSpace() - 1)\r\n",
    "\r\n",
    "def evaluate_policy(env, policy, episodes = 1000, view = True, print_ep_reward=False):\r\n",
    "    # renders the environment and agent at each step if view is True\r\n",
    "    # this takes up time so if you want it to be quicker just set view to False\r\n",
    "    total_evaluation_reward = 0\r\n",
    "    for i in range(episodes):                   # iterating over a certain number of episodes \r\n",
    "        env.reset()\r\n",
    "        episode_reward = 0\r\n",
    "        done = False\r\n",
    "        state, reward, done, info = env.step(policy)\r\n",
    "        episode_reward += reward\r\n",
    "        total_evaluation_reward += reward\r\n",
    "        while not done:\r\n",
    "            state, reward, done, info = env.step(policy)        # perform random step\r\n",
    "            episode_reward += reward\r\n",
    "            total_evaluation_reward += reward\r\n",
    "            if view == True:\r\n",
    "                print(\"state\", state)\r\n",
    "                env.render()\r\n",
    "                time.sleep(0.5)\r\n",
    "                clear_output(wait=True)\r\n",
    "        if print_ep_reward == True:\r\n",
    "            print(\"Total reward for trial = \", episode_reward)\r\n",
    "    return total_evaluation_reward / episodes                   # return average reward over all episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for trial =  927\n",
      "Total reward for trial =  -1001\n",
      "Total reward for trial =  -1000\n",
      "Total reward for trial =  858\n",
      "Total reward for trial =  970\n",
      "Total reward for trial =  954\n",
      "Total reward for trial =  -1019\n",
      "Total reward for trial =  -1085\n",
      "Total reward for trial =  -1004\n",
      "Total reward for trial =  -1021\n",
      "Average reward: -242.1\n"
     ]
    }
   ],
   "source": [
    "env_size = 11\r\n",
    "simplicity = 4\r\n",
    "stronghold = Stronghold(env_size, simplicity)            \r\n",
    "print(\"Average reward:\", evaluate_policy(stronghold, random_policy(), episodes=10, view=False, print_ep_reward=True)) # <- set view to True if you want the environment to show during evaluation and print to see the reward per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: -772.561\n"
     ]
    }
   ],
   "source": [
    "env_size = 11\r\n",
    "simplicity = 6\r\n",
    "stronghold = Stronghold(env_size, simplicity)            \r\n",
    "print(\"Average reward:\", evaluate_policy(stronghold, random_policy(), episodes=1000, view=False, print_ep_reward=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: -1043.9177\n"
     ]
    }
   ],
   "source": [
    "env_size = 21\r\n",
    "simplicity = 6\r\n",
    "stronghold = Stronghold(env_size, simplicity)            \r\n",
    "print(\"Average reward:\", evaluate_policy(stronghold, random_policy(), episodes=10000, view=False, print_ep_reward=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "name": "python387jvsc74a57bd02db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}