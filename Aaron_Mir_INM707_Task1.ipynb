{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# INM707 Coursework\n",
    "### Aaron Mir (Student Number: 160001207)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##--------------------Coding References---------------------## <img src=\"All_Tasks.png\" alt=\"All_TasksOverview\" width=\"700\"/>  <img src=\"Task_1.png\" alt=\"Task_1\" width=\"700\"/>\n",
    "# Percentage of borrowed code: X% - \n",
    "# [1] "
   ]
  },
  {
   "source": [
    "***\n",
    "## Stronghold\n",
    "\n",
    "The robot agent is preparing for an upcoming mission in which they are to infiltrate an enemy stronghold to gather intelligence on a potential coup d'Ã©tat. The enemy land is in the form of an NxN grid with each grid containing one stronghold of size N x N/2 (starting on either side of the middle column (randomly decided)) and a wide river surrounding the entire enemy land. The entire land is very misty, therefore the robot may not step where it is supposed to. Each move has a transition probability associated with it (0.8 to move to the chosen state or 0.2 to move to either side) (stochasticity). The stronghold has as many entrances from the mainland as (1/4)N and contains (1/5)N dangerous enemy combatants (at random positions) within the stronghold that move around randomly per step (stochasticity).  The amount of shore that has traps on it is (1/4)N x rows of shore. The column immediately beside the stronghold is normal land to prevent entrance blocking. The goal of the agent is to infiltrate the stronghold and gather the intelligence without being killed by enemy combatants or booby traps as fast as possible.\n",
    "\n",
    "This is a type of gridworld environment. The size (number of states) can be controlled by adjusting the grid dimensions. The environment is intended to model the process of navigating a frozen lake, while avoiding falling into holes with the objective of reaching a goal location. \n",
    "\n",
    "The environment is defined as follows:\n",
    "\n",
    "- The environment is a rectangular grid of states/cells. There are five different types of cells as indicated by the following cell labels: \n",
    "\n",
    "    - _ labels cells that are safe to step on i.e. normal land [0]\n",
    "\n",
    "    - X labels the cells that are walls i.e. river or wall and if the agent enters a wall cell, there is a penalty of -1 [1]\n",
    "    \n",
    "    - A labels the cell the agent is on, starts on a random cell on the shore [2]\n",
    "    \n",
    "    - T labels booby-trap cells and if the agent enters a booby-trap cell there is a pentalty of -1000 and the episode ends [3]\n",
    "\n",
    "    - E labels enemy cells and if the agent enters an enemy cell there is a pentalty of -1000 and the episode ends [4]\n",
    "\n",
    "    - I labels the intelligence cell and when reached gives a reward of 1000 and the episode ends [5]\n",
    "\n",
    "- There are four possible actions (Up, Down, Left, Right). \n",
    "\n",
    "- The transition function moves the agent in the expected direction with 0.8 probability, and there is a 0.2 probability of transitioning to one of the sides.\n",
    "\n",
    "- There is a reward of -1 for each action taken by the agent, which is intended to encourage the agent to reach the goal as fast as possible. If the agent runs out of time, the episode ends. \n",
    "\n",
    "- Episodes end whenever the agent falls in a hole or reaches the goal. An end-of-episode is modeled by transitioning to a zero-reward terminal state (all actions lead to that state). \n",
    "\n",
    "State of the agent: Governed by the index of the cell it is on.\n",
    "\n",
    "States of the environment: Governed by the index of the agent and the index of moving enemies.\n",
    "\n",
    "Number of states of the environment is given by the size of the grid and the size of the stronghold in which enemies can move. e.g. for a 10x10 grid with a stronghold size of 6x3 and 6 enemies, the number of states is 100 - 6x3-1 +  (6x3-1 (because of starting state)/6)  - 1 (because of the intelligence in the stronghold)\n",
    "\n",
    "Rewards/Penalties: +1000 for getting intelligence, -1000 for getting hurt by a combatant or booby trap, -1 for moving into a wall or water, -1 per transition because of fuel constraints\n",
    "\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Observations will be fed into the dqn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Checklist:\n",
    "\n",
    "The MDP consists of states, a transition probability, a reward function, and also actions\n",
    "\n",
    "    1. Estabilish states (done, represented by the cell the agent is on)\n",
    "    2. Estabilish transition probability matrix (done)\n",
    "    3. Estabilish rewards matrix (done)\n",
    "    4. Establish return G (Not really needed?)\n",
    "    5. Sort out enemy movement (done, enemies move within the stronghold)\n",
    "    6. Implement partial observability (done)\n",
    "    7. Improve visualisation (done)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=sys.maxsize, precision = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the state of the agent is entirely described by either the coordinates of the cell it is on, or the cell numbe.\n",
    "# the value function (can be represented as a dictionary, or an array) that maps the state to the value of the state. As we don't know the true value at the beginning, it will be initialized at 1.\n",
    "# the current state is given by self.position_agent gives the state i.e. its index\n",
    "# the actions will be the direct cells that an agent can go to from a particular cell or up down left right\n",
    "# the rewards will be given to a robot if a cell/state is directly reachable from the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stronghold(): # discrete.DiscreteEnv\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.actions = {0: 'Up', 1: 'Down', 2: 'Left', 3: 'Right'}\n",
    "        self.land = self.env_gen()\n",
    "        self.position_agent = None                                 # initial position of the agent will be decided by resetting the environment\n",
    "        self.time_elapsed = 0                                      # run time\n",
    "        self.time_limit = self.size**2\n",
    "        self.numActions, self.numStates = len(self.actions), size * size\n",
    "        self.prob_of_tripping = [0.7, 0.1, 0.1, 0.1]               # 0.7 probability of going to where it chooses and 0.3 to go to differently\n",
    "        self.P = {s:{a:[] for a in range(self.numActions)} for s in range(self.numStates)}      # dictionary mapping states and actions\n",
    "        self.transitionProb = np.zeros((self.numActions, self.numStates + 1, self.numStates + 1))\n",
    "        self.transitionReward = np.zeros((self.numStates + 1, self.numActions))\n",
    "        self.fill_transitions_probability()\n",
    "        self.dict_map_display={ 0:'_',          # normal land\n",
    "                                1:'X',          # river/wall\n",
    "                                2:'A',          # agent\n",
    "                                3:'T',          # trap\n",
    "                                4:'E',          # enemy\n",
    "                                5:'I'}          # intelligence\n",
    "                    \n",
    "    def env_gen(self):\n",
    "        land = np.zeros((self.size, self.size))\n",
    "        land[0,:] = 1                                               # establish the river\n",
    "        land[:,0] = 1\n",
    "        land[self.size-1,:] = 1\n",
    "        land[:,self.size-1] = 1\n",
    "        self.column_choice = np.random.choice((self.size//2-1, self.size//2+1)) # random choice whether stronghold starts from the left or right of the 'central' column\n",
    "        land[1:self.size-1, self.column_choice] = 1\n",
    "        if self.column_choice == self.size//2-1:                              # if stronghold is on left\n",
    "            land[1:self.size-1, 0] = 1                                        # establish the walls of stronghold\n",
    "            for col in land[1:self.size-1, self.column_choice+2:self.size-1].T:\n",
    "                traps = []\n",
    "                for i in range(int(np.round(1/4*len(col)))):        # make as many traps as 1/4 of the length of each column in the shore\n",
    "                    trap = np.random.choice(np.setdiff1d(range(len(col)), traps))\n",
    "                    col[trap] = 3\n",
    "                    traps.append(trap)\n",
    "            for col in land[1:self.size-1, 1:self.column_choice].T:\n",
    "                enemies = []\n",
    "                for i in range(int(np.round(1/5*len(col)))):        # populate enemies randomly inside the stronghold\n",
    "                    enemy = np.random.choice(np.setdiff1d(range(len(col)), enemies))\n",
    "                    col[enemy] = 4\n",
    "                    enemies.append(enemy)\n",
    "            intel_row = np.random.randint(1, len(land[1:self.size-1]))\n",
    "            intel_col = np.random.randint(1, len(land[1:self.column_choice-1]))\n",
    "            land[intel_row+1, intel_col] = 5\n",
    "            self.position_intel = [intel_row+1, intel_col]            # randomly insert intelligence into stronghold\n",
    "        else:                                                              # if stronghold is on right                          \n",
    "            land[1:self.size-1, self.size-1] = 1                            # establish the walls of stronghold\n",
    "            for col in land[1:self.size-1, 1:self.column_choice-1].T:\n",
    "                traps = []\n",
    "                for i in range(int(np.round(1/4*len(col)))):        # make as many traps as 1/4 of the length of each column in the shore\n",
    "                    trap = np.random.choice(np.setdiff1d(range(len(col)), traps))\n",
    "                    col[trap] = 3\n",
    "                    traps.append(trap)\n",
    "            for col in land[1:self.size-1, self.column_choice+1:self.size-1].T:\n",
    "                enemies = []\n",
    "                for i in range(int(np.round(1/5*len(col)))):        # populate enemies randomly inside the stronghold\n",
    "                    enemy = np.random.choice(np.setdiff1d(range(len(col)), enemies))\n",
    "                    col[enemy] = 4\n",
    "                    enemies.append(enemy)\n",
    "            intel_row = np.random.randint(1, len(land[1:self.size-1]))              \n",
    "            intel_col = np.random.randint(1, len(land[self.column_choice+1:self.size-1]))\n",
    "            land[intel_row+1, self.column_choice+intel_col] = 5\n",
    "            self.position_intel = [intel_row+1, self.column_choice+intel_col]             # randomly insert intelligence into stronghold\n",
    "        entrances = []\n",
    "        for i in range(int(np.round(1/4*len(land[2:self.size-2, self.column_choice])))):        # make as many entrances as 1/4 of the length of the front wall \n",
    "            entrance = np.random.choice(np.setdiff1d(range(len(land[2:self.size-2, self.column_choice])), entrances))\n",
    "            land[2:self.size-2, self.column_choice][entrance] = 0\n",
    "            entrances.append(entrance)\n",
    "        return land\n",
    "\n",
    "    def to_state(self, row, col):\n",
    "            return row*self.size + col\n",
    "\n",
    "    def inc(self, row, col, a):\n",
    "        if a == 0:\n",
    "            row = max(row-1,0)\n",
    "        elif a == 1:\n",
    "            row = min(row+1, self.size-1)       \n",
    "        elif a == 2:\n",
    "            col = max(col-1,0)\n",
    "        elif a == 3:\n",
    "            col = min(col+1,self.size-1)\n",
    "        return (row, col)\n",
    "    \n",
    "    def fill_transitions_probability(self):\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size):\n",
    "                s = self.to_state(row, col)\n",
    "                for a in range(4):\n",
    "                    trans_prob = self.P[s][a]\n",
    "                    number = self.land[row, col]\n",
    "                    if number == 3: # trap\n",
    "                        trans_prob.append((1.0, s, 0, True))\n",
    "                        self.transitionProb[a, s, self.numStates] = 1.0\n",
    "                        self.transitionReward[s, a] = -1000\n",
    "                    elif number == 4: # enemy\n",
    "                        trans_prob.append((1.0, s, 0, True))\n",
    "                        self.transitionProb[a, s, self.numStates] = 1.0\n",
    "                        self.transitionReward[s, a] = -1000\n",
    "                    elif number == 5: # intelligence\n",
    "                        trans_prob.append((1.0, s, 0, True))\n",
    "                        self.transitionProb[a, s, self.numStates] = 1.0\n",
    "                        self.transitionReward[s, a] = 1000\n",
    "                    else:\n",
    "                        for b, p in zip([a, (a+1)%4, (a+2)%4, (a+3)%4], self.prob_of_tripping):\n",
    "                            newrow, newcol = self.inc(row, col, b)\n",
    "                            newstate = self.to_state(newrow, newcol)\n",
    "                            newnumber = self.land[newrow][newcol]\n",
    "                            done = False\n",
    "                            if newnumber == 3:                  # if enters trap\n",
    "                                rew = -1000\n",
    "                            elif newnumber == 4:                # if enemy kills\n",
    "                                rew = -1000\n",
    "                            elif newnumber == 5:                # if intelligence is caught\n",
    "                                rew = 1000\n",
    "                            else:\n",
    "                                rew = -1                        # penalty for time-step               \n",
    "                            trans_prob.append((p, newstate, rew, done))\n",
    "                            self.transitionProb[a, s, newstate] += p\n",
    "                            self.transitionReward[s, a] = -1 \n",
    "    \n",
    "    def getSuccessors(self, s, a): \n",
    "        # Take a state and an action as input, and return a list of pairs, where each pair (s', p) is a successor state s' with non-zero probability and p is the probability of transitioning to p.\n",
    "        idx = [0, 3, 1, 2] # up left right down rearrange to up down left right [0, 3, 1, 2]    left right up down [1, 2, 0, 3]\n",
    "        next_states = np.nonzero(self.transitionProb[a, s, :]) # np.nonzero(self.transitionProb[a, s, :])\n",
    "        probs = self.transitionProb[a, s, next_states]\n",
    "        if np.size(next_states[0]) == 1:\n",
    "            return next_states[0]\n",
    "        else:        \n",
    "            return [(s,p) for s,p in zip(next_states[0][idx], probs[0][idx])]  \n",
    "\n",
    "    def gettransitionProb(self, s, a, ns):\n",
    "        # Take a state, an action, a next state as input, and return the probability of the transition \n",
    "        return self.transitionProb[a, s, ns]\n",
    "\n",
    "    def getReward(self, s, a):\n",
    "        # Take a state and an action as input, and return the reward of that.\n",
    "        return self.transitionReward[s, a]\n",
    "\n",
    "    def getStateSpace(self):\n",
    "        return self.transitionProb.shape[1]\n",
    "    \n",
    "    def getActionSpace(self):\n",
    "        return self.transitionProb.shape[0]\n",
    "\n",
    "    def normal_shore(self, n_cells):\n",
    "        if self.column_choice == self.size//2-1:\n",
    "            empty_cells_coord = np.where(self.land[1:self.size-1, self.column_choice+1:self.size-1] == 0)\n",
    "            selected_indices = np.random.choice(np.arange(len(empty_cells_coord[0])), n_cells)\n",
    "            selected_coordinates = empty_cells_coord[0][selected_indices]+1, empty_cells_coord[1][selected_indices]+len(self.land[1][:self.column_choice+1])\n",
    "        if self.column_choice == self.size//2+1:\n",
    "            empty_cells_coord = np.where(self.land[1:self.size-1, 1:self.column_choice] == 0)\n",
    "            selected_indices = np.random.choice(np.arange(len(empty_cells_coord[0])), n_cells)\n",
    "            selected_coordinates = empty_cells_coord[0][selected_indices]+1, empty_cells_coord[1][selected_indices]+1\n",
    "        if n_cells == 1:\n",
    "            return np.asarray(selected_coordinates).reshape(2,)\n",
    "        return selected_coordinates\n",
    "\n",
    "    def step(self, action): # action is 0, 1, 2, 3 for up, down, left, right\n",
    "        for i, j in zip(*np.where(self.land == 4)):     # enemies move randomly - they do not move if their choice is a wall, the intelligence, another enemy or the stronghold entrance column\n",
    "            move = np.random.choice(('up', 'down', 'left', 'right'))\n",
    "            if move == 'up' and self.land[i-1, j] != 1 and self.land[i-1, j] != 4 \\\n",
    "                and self.land[i-1, j] != 5:\n",
    "                self.land[i, j] = 0\n",
    "                self.land[i-1, j] = 4\n",
    "            if move == 'down' and self.land[i+1, j] != 1 and self.land[i+1, j] != 4 \\\n",
    "                and self.land[i+1, j] != 5:\n",
    "                self.land[i, j] = 0\n",
    "                self.land[i+1][j] = 4\n",
    "            if move == 'left' and self.land[i, j-1] != 1 and self.land[i, j-1] != 4 \\\n",
    "                and self.land[i, j-1] != 5 and j-1 != self.column_choice:\n",
    "                self.land[i, j] = 0\n",
    "                self.land[i, j-1] = 4\n",
    "            if move == 'right' and self.land[i, j+1] != 1 and self.land[i, j+1] != 4 \\\n",
    "                and self.land[i, j+1] != 5 and j+1 != self.column_choice:\n",
    "                self.land[i, j] = 0\n",
    "                self.land[i, j+1] = 4\n",
    "\n",
    "        #print(action)\n",
    "        current_position = np.array((self.position_agent[0], self.position_agent[1])) # saving the current position and state in case agent hits a wall\n",
    "        current_state = self.agent_state\n",
    "        #print(current_state)                                              \n",
    "        options = self.getSuccessors(current_state, action)                           # chosen action gets 0.7 prob \n",
    "        if np.size(options) != 1:\n",
    "            #print(options)\n",
    "            probs = [i[1] for i in options]                                               #  list of probabilities\n",
    "            #print(probs)\n",
    "            choice = np.random.choice((0, 1, 2, 3), p=probs)                              #  make a probability choice\n",
    "            state_choice = options[choice][0]\n",
    "            #print(self.P[self.agent_state][action])                                        \n",
    "            ind = np.where(self.P[self.agent_state][action][:][:] == state_choice)[0][0]\n",
    "            prob, new_state, reward, done = self.P[self.agent_state][action][ind]    # new prob, state, reward, done for the choice                 \n",
    "\n",
    "            self.agent_state = new_state                                                # agent state and cell index position gets updated\n",
    "            if choice == 0:                                                             # up                                   \n",
    "                self.position_agent[0] -= 1\n",
    "            if choice == 1:                                                             # down\n",
    "                self.position_agent[0] += 1\n",
    "            if choice == 2:                                                             # left\n",
    "                self.position_agent[1] -= 1 \n",
    "            if choice == 3:                                                             # right\n",
    "                self.position_agent[1] += 1\n",
    "        \n",
    "            if self.land[self.position_agent[0], self.position_agent[1]] == 1:          # condition where they walk into a wall\n",
    "                self.position_agent = current_position                                  # state and position of agent = old state and position\n",
    "                new_state = current_state\n",
    "                self.agent_state = new_state\n",
    "                reward -= 1                                                             # additional penalty for bumping into wall\n",
    "        else:\n",
    "            prob, new_state, reward, done = self.P[self.agent_state][action][0]\n",
    "            if self.time_elapsed == self.time_limit:                                    # time-limit termination condition\n",
    "                done = True\n",
    "                print ('Time ran out')\n",
    "            else:\n",
    "                self.time_elapsed += 1                                                  # update time\n",
    "                            \n",
    "        # observations = self.observations()                  \n",
    "        # self.render()                                                             # update visualisation\n",
    "        # print((int(new_state), reward, done, {\"prob\" : prob}))\n",
    "        return (int(new_state), reward, done, {\"prob\" : prob})                      # return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.time_elapsed = 0                                              # put time_elapsed to 0\n",
    "        self.position_agent = np.asarray(self.normal_shore(1))             # position of the agent is a random cell on the shore\n",
    "        self.agent_state = self.to_state(self.position_agent[0], self.position_agent[1])\n",
    "        #observations = self.agent_state                                    # calculate observations i.e. current state\n",
    "        #return observations\n",
    "\n",
    "    def render(self):                                                       # display\n",
    "        envir_with_agent = self.land.copy()\n",
    "        envir_with_agent[self.position_agent[0], self.position_agent[1]] = 2\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        im = ax.imshow(envir_with_agent) \n",
    "\n",
    "        ax.set_xticks(np.arange(len(envir_with_agent)))\n",
    "        ax.set_yticks(np.arange(len(envir_with_agent)))\n",
    "        \n",
    "        for i in range(len(envir_with_agent)):                              # loop over data dimensions and create text annotations\n",
    "            for j in range(len(envir_with_agent)):\n",
    "                if envir_with_agent[i, j] == 0:\n",
    "                    text = \"\"\n",
    "                elif envir_with_agent[i, j] == 1:\n",
    "                    text = \"X\"\n",
    "                elif envir_with_agent[i, j] == 2:\n",
    "                    text = \"A\"\n",
    "                elif envir_with_agent[i, j] == 3:\n",
    "                    text = \"T\"\n",
    "                elif envir_with_agent[i, j] == 4:\n",
    "                    text = \"E\"\n",
    "                elif envir_with_agent[i, j] == 5:\n",
    "                    text = \"I\"\n",
    "                else:\n",
    "                    text = \"\"    \n",
    "                text_cell = ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"r\", fontsize='xx-large')\n",
    "\n",
    "        ax.set_title(\"Stronghold\", fontsize=15) # 35 for 10x10 \n",
    "        fig.tight_layout()\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 360x360 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"335.197656pt\" version=\"1.1\" viewBox=\"0 0 317.8 335.197656\" width=\"317.8pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-03-23T19:16:40.104028</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 335.197656 \r\nL 317.8 335.197656 \r\nL 317.8 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g clip-path=\"url(#pc822d6d895)\">\r\n    <image height=\"304\" id=\"imagedb86658df7\" transform=\"scale(1 -1)translate(0 -304)\" width=\"304\" x=\"7.2\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAATAAAAEwCAYAAAAw+y3zAAAE+klEQVR4nO3WsY1UZxiG0R/vrcJkU8GV0DpwhJaAHqYMEifOEY1MD04IIVgh3Qomw4FrYAU9IHs/PZ5zGng/6V49+l+8fvP++wII+mX6AICfJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkLWNDX/8MjV9k66XfWT3dD5Gdr89vBrZXWvu374/nkZ211rr87vfRna9wIAsAQOyBAzIEjAgS8CALAEDsgQMyBIwIEvAgCwBA7IEDMgSMCBLwIAsAQOyBAzIEjAgS8CALAEDsgQMyBIwIEvAgCwBA7IEDMgSMCBLwIAsAQOyBAzIEjAgS8CALAEDsrbpA3gep/MxfQL/scf9bm78YWbWCwzIEjAgS8CALAEDsgQMyBIwIEvAgCwBA7IEDMgSMCBLwIAsAQOyBAzIEjAgS8CALAEDsgQMyBIwIEvAgCwBA7IEDMgSMCBLwIAsAQOyBAzIEjAgS8CALAEDsgQMyNqmhq+XfWp6nc7H2Dbw7/ECA7IEDMgSMCBLwIAsAQOyBAzIEjAgS8CALAEDsgQMyBIwIEvAgCwBA7IEDMgSMCBLwIAsAQOyBAzIEjAgS8CALAEDsgQMyBIwIEvAgCwBA7IEDMgSMCBLwIAsAQOyBAzI2qaGT+djanpdL/vY9q2Z/M5T7o+nkd3H/W5kd5IXGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQtU0NXy/71PQ6nY+xbf7/Hve76RNuhhcYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJC1TQ2fzsfUNDdg+/hl+oRnd388jW1/fjez6wUGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJC1TR8w4XrZp094dqfzMbL79Y/fR3Zffvg0srvWWn/9fYzsvv11H9lda631MDPrBQZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZG3TB0w4nY/pE27Gyw+fRna/Pbwa2V1rrT//eRrbvjVeYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWS9ev3n/ffoIgJ/hBQZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZP0AxKs4OZ/hGZEAAAAASUVORK5CYII=\" y=\"-23.997656\"/>\r\n   </g>\r\n   <g id=\"text_1\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(16.4516 44.535856)scale(0.1728 -0.1728)\">\r\n     <defs>\r\n      <path d=\"M 6.296875 72.90625 \r\nL 16.890625 72.90625 \r\nL 35.015625 45.796875 \r\nL 53.21875 72.90625 \r\nL 63.8125 72.90625 \r\nL 40.375 37.890625 \r\nL 65.375 0 \r\nL 54.78125 0 \r\nL 34.28125 31 \r\nL 13.625 0 \r\nL 2.984375 0 \r\nL 29 38.921875 \r\nz\r\n\" id=\"DejaVuSans-88\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_2\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(46.7916 44.535856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_3\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(77.1316 44.535856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_4\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(107.4716 44.535856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_5\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(137.8116 44.535856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_6\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(168.1516 44.535856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_7\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(198.4916 44.535856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_8\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(228.8316 44.535856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_9\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(259.1716 44.535856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_10\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(289.5116 44.535856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_11\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(16.4516 74.875856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_12\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(198.4916 74.875856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_13\">\r\n    <!-- E -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(229.2906 74.875856)scale(0.1728 -0.1728)\">\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 55.90625 72.90625 \r\nL 55.90625 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.015625 \r\nL 54.390625 43.015625 \r\nL 54.390625 34.71875 \r\nL 19.671875 34.71875 \r\nL 19.671875 8.296875 \r\nL 56.78125 8.296875 \r\nL 56.78125 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-69\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-69\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_14\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(289.5116 74.875856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_15\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(16.4516 105.215856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_16\">\r\n    <!-- T -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(77.77285 105.215856)scale(0.1728 -0.1728)\">\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_17\">\r\n    <!-- T -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(108.11285 105.215856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_18\">\r\n    <!-- A -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(168.1597 105.215856)scale(0.1728 -0.1728)\">\r\n     <defs>\r\n      <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-65\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_19\">\r\n    <!-- I -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(232.2012 105.215856)scale(0.1728 -0.1728)\">\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-73\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-73\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_20\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(289.5116 105.215856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_21\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(16.4516 135.555856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_22\">\r\n    <!-- E -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(259.6306 135.555856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-69\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_23\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(289.5116 135.555856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_24\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(16.4516 165.895856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_25\">\r\n    <!-- T -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(47.43285 165.895856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_26\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(198.4916 165.895856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_27\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(289.5116 165.895856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_28\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(16.4516 196.235856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_29\">\r\n    <!-- T -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(77.77285 196.235856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_30\">\r\n    <!-- T -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(108.11285 196.235856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_31\">\r\n    <!-- T -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(138.45285 196.235856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_32\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(198.4916 196.235856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_33\">\r\n    <!-- E -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(229.2906 196.235856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-69\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_34\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(289.5116 196.235856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_35\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(16.4516 226.575856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_36\">\r\n    <!-- T -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(47.43285 226.575856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_37\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(198.4916 226.575856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_38\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(289.5116 226.575856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_39\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(16.4516 256.915856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_40\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(198.4916 256.915856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_41\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(289.5116 256.915856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_42\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(16.4516 287.255856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_43\">\r\n    <!-- T -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(138.45285 287.255856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_44\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(198.4916 287.255856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_45\">\r\n    <!-- E -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(259.6306 287.255856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-69\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_46\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(289.5116 287.255856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_47\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(16.4516 317.595856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_48\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(46.7916 317.595856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_49\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(77.1316 317.595856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_50\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(107.4716 317.595856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_51\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(137.8116 317.595856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_52\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(168.1516 317.595856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_53\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(198.4916 317.595856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_54\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(228.8316 317.595856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_55\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(259.1716 317.595856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_56\">\r\n    <!-- X -->\r\n    <g style=\"fill:#ff0000;\" transform=\"translate(289.5116 317.595856)scale(0.1728 -0.1728)\">\r\n     <use xlink:href=\"#DejaVuSans-88\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_57\">\r\n    <!-- Stronghold -->\r\n    <g transform=\"translate(117.993359 18.597656)scale(0.15 -0.15)\">\r\n     <defs>\r\n      <path d=\"M 53.515625 70.515625 \r\nL 53.515625 60.890625 \r\nQ 47.90625 63.578125 42.921875 64.890625 \r\nQ 37.9375 66.21875 33.296875 66.21875 \r\nQ 25.25 66.21875 20.875 63.09375 \r\nQ 16.5 59.96875 16.5 54.203125 \r\nQ 16.5 49.359375 19.40625 46.890625 \r\nQ 22.3125 44.4375 30.421875 42.921875 \r\nL 36.375 41.703125 \r\nQ 47.40625 39.59375 52.65625 34.296875 \r\nQ 57.90625 29 57.90625 20.125 \r\nQ 57.90625 9.515625 50.796875 4.046875 \r\nQ 43.703125 -1.421875 29.984375 -1.421875 \r\nQ 24.8125 -1.421875 18.96875 -0.25 \r\nQ 13.140625 0.921875 6.890625 3.21875 \r\nL 6.890625 13.375 \r\nQ 12.890625 10.015625 18.65625 8.296875 \r\nQ 24.421875 6.59375 29.984375 6.59375 \r\nQ 38.421875 6.59375 43.015625 9.90625 \r\nQ 47.609375 13.234375 47.609375 19.390625 \r\nQ 47.609375 24.75 44.3125 27.78125 \r\nQ 41.015625 30.8125 33.5 32.328125 \r\nL 27.484375 33.5 \r\nQ 16.453125 35.6875 11.515625 40.375 \r\nQ 6.59375 45.0625 6.59375 53.421875 \r\nQ 6.59375 63.09375 13.40625 68.65625 \r\nQ 20.21875 74.21875 32.171875 74.21875 \r\nQ 37.3125 74.21875 42.625 73.28125 \r\nQ 47.953125 72.359375 53.515625 70.515625 \r\nz\r\n\" id=\"DejaVuSans-83\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-83\"/>\r\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"102.685547\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"141.548828\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"202.730469\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"266.109375\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"329.585938\" xlink:href=\"#DejaVuSans-104\"/>\r\n     <use x=\"392.964844\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"454.146484\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"481.929688\" xlink:href=\"#DejaVuSans-100\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pc822d6d895\">\r\n   <rect height=\"303.4\" width=\"303.4\" x=\"7.2\" y=\"24.597656\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "stronghold = Stronghold(10)\n",
    "stronghold.reset()\n",
    "stronghold.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1021\n",
      "New Trial\n",
      "Total reward for trial =  -1039\n",
      "New Trial\n",
      "Total reward for trial =  -1111\n",
      "New Trial\n",
      "Total reward for trial =  -1035\n",
      "New Trial\n",
      "Total reward for trial =  -1037\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1060\n",
      "New Trial\n",
      "Total reward for trial =  -1075\n",
      "New Trial\n",
      "Total reward for trial =  -1081\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1022\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1072\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1049\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1038\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1038\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1069\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1030\n",
      "New Trial\n",
      "Total reward for trial =  -1033\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1018\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1035\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1027\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1077\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1107\n",
      "New Trial\n",
      "Total reward for trial =  -1014\n",
      "New Trial\n",
      "Total reward for trial =  -1042\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1026\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1014\n",
      "New Trial\n",
      "Total reward for trial =  -1066\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1047\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1040\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1091\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1044\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1054\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1057\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1030\n",
      "New Trial\n",
      "Total reward for trial =  -1031\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1074\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1018\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1051\n",
      "New Trial\n",
      "Total reward for trial =  -1025\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1057\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1037\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1087\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1036\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1021\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1061\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1031\n",
      "New Trial\n",
      "Total reward for trial =  -1015\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1037\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1022\n",
      "New Trial\n",
      "Total reward for trial =  -1067\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1067\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1052\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1048\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1050\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1064\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1184\n",
      "New Trial\n",
      "Total reward for trial =  -1171\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1119\n",
      "New Trial\n",
      "Total reward for trial =  -1022\n",
      "New Trial\n",
      "Total reward for trial =  -1071\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1068\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1049\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1050\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1027\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1026\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1027\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1061\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1027\n",
      "New Trial\n",
      "Total reward for trial =  -1032\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1031\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1057\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1069\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1029\n",
      "New Trial\n",
      "Total reward for trial =  -1043\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1027\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1033\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1125\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1204\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1014\n",
      "New Trial\n",
      "Total reward for trial =  -1060\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1035\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1050\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1029\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1014\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1048\n",
      "New Trial\n",
      "Total reward for trial =  -1055\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1020\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1031\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1062\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1046\n",
      "New Trial\n",
      "Total reward for trial =  -1029\n",
      "New Trial\n",
      "Total reward for trial =  -1134\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1038\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1050\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1050\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1071\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1085\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1037\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1157\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1134\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1018\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1060\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1018\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1014\n",
      "New Trial\n",
      "Total reward for trial =  -1025\n",
      "New Trial\n",
      "Total reward for trial =  -1015\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1056\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1044\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1048\n",
      "New Trial\n",
      "Total reward for trial =  -1087\n",
      "New Trial\n",
      "Total reward for trial =  -1060\n",
      "New Trial\n",
      "Total reward for trial =  -1100\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1034\n",
      "New Trial\n",
      "Total reward for trial =  -1035\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1026\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1079\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1054\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1052\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1034\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1039\n",
      "New Trial\n",
      "Total reward for trial =  -1044\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1024\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1015\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1037\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1080\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1058\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1026\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1034\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1089\n",
      "New Trial\n",
      "Total reward for trial =  -1024\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1014\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1153\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1051\n",
      "New Trial\n",
      "Total reward for trial =  -1050\n",
      "New Trial\n",
      "Total reward for trial =  -1077\n",
      "New Trial\n",
      "Total reward for trial =  -1027\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1050\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1060\n",
      "New Trial\n",
      "Total reward for trial =  -1037\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1053\n",
      "New Trial\n",
      "Total reward for trial =  -1059\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1063\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1024\n",
      "New Trial\n",
      "Total reward for trial =  -1063\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1073\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1024\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1070\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1089\n",
      "New Trial\n",
      "Total reward for trial =  -1030\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1021\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1059\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1040\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1045\n",
      "New Trial\n",
      "Total reward for trial =  -1044\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1038\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1031\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1049\n",
      "New Trial\n",
      "Total reward for trial =  -1097\n",
      "New Trial\n",
      "Total reward for trial =  -1023\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1069\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1025\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1024\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1046\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1018\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1090\n",
      "New Trial\n",
      "Total reward for trial =  -1211\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1051\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1036\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1038\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1083\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1015\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1076\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1043\n",
      "New Trial\n",
      "Total reward for trial =  -1058\n",
      "New Trial\n",
      "Total reward for trial =  -1031\n",
      "New Trial\n",
      "Total reward for trial =  -1020\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1059\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1024\n",
      "New Trial\n",
      "Total reward for trial =  -1018\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1109\n",
      "New Trial\n",
      "Total reward for trial =  -1025\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1073\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1023\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1049\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1021\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1056\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1022\n",
      "New Trial\n",
      "Total reward for trial =  -1212\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1022\n",
      "New Trial\n",
      "Total reward for trial =  -1049\n",
      "New Trial\n",
      "Total reward for trial =  -1170\n",
      "New Trial\n",
      "Total reward for trial =  -1020\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1022\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1033\n",
      "New Trial\n",
      "Total reward for trial =  -1048\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1030\n",
      "New Trial\n",
      "Total reward for trial =  -1110\n",
      "New Trial\n",
      "Total reward for trial =  -1022\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1020\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1024\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1049\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1020\n",
      "New Trial\n",
      "Total reward for trial =  -1097\n",
      "New Trial\n",
      "Total reward for trial =  -1033\n",
      "New Trial\n",
      "Total reward for trial =  -1024\n",
      "New Trial\n",
      "Total reward for trial =  -1115\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1042\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1052\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1056\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1035\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1036\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1021\n",
      "New Trial\n",
      "Total reward for trial =  -1015\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1015\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1032\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1047\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1023\n",
      "New Trial\n",
      "Total reward for trial =  -1023\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1045\n",
      "New Trial\n",
      "Total reward for trial =  -1025\n",
      "New Trial\n",
      "Total reward for trial =  -1031\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1033\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1027\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1029\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1041\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1078\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1070\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1023\n",
      "New Trial\n",
      "Total reward for trial =  -1103\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1120\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1129\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1035\n",
      "New Trial\n",
      "Total reward for trial =  -1151\n",
      "New Trial\n",
      "Total reward for trial =  -1116\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1039\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1098\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1015\n",
      "New Trial\n",
      "Total reward for trial =  -1036\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1018\n",
      "New Trial\n",
      "Total reward for trial =  -1066\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1098\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1022\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1021\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1043\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1037\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1026\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1020\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1143\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1030\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1068\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1026\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1139\n",
      "New Trial\n",
      "Total reward for trial =  -1037\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1020\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1037\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1006\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1008\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1046\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1149\n",
      "New Trial\n",
      "Total reward for trial =  -1075\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1020\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1059\n",
      "New Trial\n",
      "Total reward for trial =  -1082\n",
      "New Trial\n",
      "Total reward for trial =  -1040\n",
      "New Trial\n",
      "Total reward for trial =  -1040\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1012\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1069\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1054\n",
      "New Trial\n",
      "Total reward for trial =  -1129\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1071\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1018\n",
      "New Trial\n",
      "Total reward for trial =  -1060\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1014\n",
      "New Trial\n",
      "Total reward for trial =  -1010\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1041\n",
      "New Trial\n",
      "Total reward for trial =  -1048\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1123\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1051\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1036\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1033\n",
      "New Trial\n",
      "Total reward for trial =  -1072\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1009\n",
      "New Trial\n",
      "Total reward for trial =  -1024\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1014\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1032\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1007\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1004\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1121\n",
      "New Trial\n",
      "Total reward for trial =  -1027\n",
      "New Trial\n",
      "Total reward for trial =  -1013\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1030\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1002\n",
      "New Trial\n",
      "Total reward for trial =  -1053\n",
      "New Trial\n",
      "Total reward for trial =  -1001\n",
      "New Trial\n",
      "Total reward for trial =  -1046\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1076\n",
      "New Trial\n",
      "Total reward for trial =  -1025\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1192\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1036\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1060\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1005\n",
      "New Trial\n",
      "Total reward for trial =  -1019\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1074\n",
      "New Trial\n",
      "Total reward for trial =  -1042\n",
      "New Trial\n",
      "Total reward for trial =  -1021\n",
      "New Trial\n",
      "Total reward for trial =  -1037\n",
      "New Trial\n",
      "Total reward for trial =  -1045\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1042\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1088\n",
      "New Trial\n",
      "Total reward for trial =  -1028\n",
      "New Trial\n",
      "Total reward for trial =  -1039\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1011\n",
      "New Trial\n",
      "Total reward for trial =  -1016\n",
      "New Trial\n",
      "Total reward for trial =  -1021\n",
      "New Trial\n",
      "Total reward for trial =  -1027\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1038\n",
      "New Trial\n",
      "Total reward for trial =  -1045\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1022\n",
      "New Trial\n",
      "Total reward for trial =  -1000\n",
      "New Trial\n",
      "Total reward for trial =  -1020\n",
      "New Trial\n",
      "Total reward for trial =  -1003\n",
      "New Trial\n",
      "Total reward for trial =  -1084\n",
      "New Trial\n",
      "Total reward for trial =  -1017\n",
      "Average reward: -1019.055\n"
     ]
    }
   ],
   "source": [
    "def random_policy():\n",
    "    action = np.random.randint(0, 3)\n",
    "    return action\n",
    "\n",
    "def evaluate_policy(env, policy, trials = 1000):\n",
    "    total_evaluation_reward = 0\n",
    "    for _ in range(trials):\n",
    "        #print('New Trial')\n",
    "        env.reset()\n",
    "        #env.render()\n",
    "        trial_reward = 0\n",
    "        done = False\n",
    "        state, reward, done, info = env.step(policy)\n",
    "        trial_reward += reward\n",
    "        total_evaluation_reward += reward\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            state, reward, done, info = env.step(policy)\n",
    "            trial_reward += reward\n",
    "            #print(\"Reward in this trial = \", trial_reward)\n",
    "            total_evaluation_reward += reward\n",
    "        print(\"Total reward for trial = \", trial_reward)\n",
    "    return total_evaluation_reward / trials\n",
    "\n",
    "stronghold = Stronghold(10)\n",
    "print(\"Average reward:\", evaluate_policy(stronghold, random_policy()))"
   ]
  },
  {
   "source": [
    "# Ignore"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policies():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def random_policy(self, observations):\n",
    "        return action\n",
    "\n",
    "        ############################################\n",
    "    def evaluate_policy(self, policy, trials = 1000):\n",
    "        total_reward = 0\n",
    "        for _ in range(trials):\n",
    "            self.env.reset()\n",
    "            done = False\n",
    "            observation, reward, done, info = self.env.step(policy[0])\n",
    "            total_reward += reward\n",
    "            while not done:\n",
    "                observation, reward, done, info = self.env.step(policy[observation])\n",
    "                total_reward += reward\n",
    "        return total_reward / trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fill_transitions_probability(self):\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size):\n",
    "                s = self.to_state(row, col)\n",
    "                for a in range(4):\n",
    "                    trans_prob = self.P[s][a]\n",
    "                    number = self.land[row, col]\n",
    "                    if number == 3: # trap\n",
    "                        trans_prob.append((1.0, s, 0, True))\n",
    "                        self.transitionProb[a, s, self.numStates] = 1.0\n",
    "                        self.transitionReward[s, a] = -1000\n",
    "                    elif number == 4: # enemy\n",
    "                        trans_prob.append((1.0, s, 0, True))\n",
    "                        self.transitionProb[a, s, self.numStates] = 1.0\n",
    "                        self.transitionReward[s, a] = 1000\n",
    "                    elif number == 5: # intelligence\n",
    "                        trans_prob.append((1.0, s, 0, True))\n",
    "                        self.transitionProb[a, s, self.numStates] = 1.0\n",
    "                        self.transitionReward[s, a] = 1000\n",
    "                    else:\n",
    "                        for b, p in zip([a, (a+1)%4, (a+2)%4, (a+3)%4], self.prob_of_tripping):\n",
    "                            newrow, newcol = self.inc(row, col, b)\n",
    "                            newstate = self.to_state(newrow, newcol)\n",
    "                            newnumber = self.land[newrow][newcol]\n",
    "                            done = False\n",
    "                            if newnumber == 1:                  # if hits wall - additional negative reward\n",
    "                                rew = -2\n",
    "                            elif newnumber == 3:                # if enters trap\n",
    "                                rew = -1000\n",
    "                            elif newnumber == 4:                # if enemy kills\n",
    "                                rew = -1000\n",
    "                            elif newnumber == 5:                # if intelligence is caught\n",
    "                                rew = 1000\n",
    "                            else:\n",
    "                                rew = -1                        # penalty for time-step\n",
    "                            trans_prob.append((p, newstate, rew, done))\n",
    "                            self.transitionProb[a, s, newstate] += p\n",
    "                            self.transitionReward[s, a] = -1                # negative reward per time-step\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def observations(self):  \n",
    "    #     proximity = self.land[self.position_agent[0]-1: self.position_agent[0]+2,\n",
    "    #                             self.position_agent[1] -1: self.position_agent[1] +2]\n",
    "    #     observations = {'relative intel coords': self.position_intel - self.position_agent, 'proximity': proximity}\n",
    "    #     return observations\n",
    "\n",
    "# def step(self, action):\n",
    "    #     # agent moves\n",
    "    #     current_position = np.array((self.position_agent[0], self.position_agent[1])) # saving the current position in case agent hits a wall\n",
    "    #     current_state = self.agent_state\n",
    "    #     reward_step = 0\n",
    "    #     if action == 0:                                          # action is 'up', 'down', 'left', or 'right'\n",
    "    #         self.position_agent[0] -= 1\n",
    "    #         self.agent_state = self.to_state(self.position_agent[0], self.position_agent[1]) \n",
    "    #     if action == 1:\n",
    "    #         self.position_agent[0] += 1\n",
    "    #         self.agent_state = self.to_state(self.position_agent[0], self.position_agent[1]) \n",
    "    #     if action == 2:\n",
    "    #         self.position_agent[1] -= 1\n",
    "    #         self.agent_state = self.to_state(self.position_agent[0], self.position_agent[1])\n",
    "    #     if action == 3:\n",
    "    #         self.position_agent[1] += 1\n",
    "    #         self.agent_state = self.to_state(self.position_agent[0], self.position_agent[1])\n",
    "\n",
    "    #     # calculate total reward\n",
    "    #     reward_step += self.getReward(self.agent_state, action)\n",
    "    #     if self.agent_state == self.intel_state:                    # termination condition\n",
    "    #         done = True\n",
    "\n",
    "        \n",
    "    #     if self.land[self.position_agent[0], self.position_agent[1]] == 1:\n",
    "    #         reward_step -= 1\n",
    "    #         self.position_agent = current_position                                                         \n",
    "    #     if self.land[self.position_agent[0], self.position_agent[1]] == 3 or self.land[self.position_agent[0], self.position_agent[1]] == 4:\n",
    "    #         reward_step -= 1000\n",
    "    #         done = True\n",
    "    #     if self.land[self.position_agent[0], self.position_agent[1]] == 5:\n",
    "    #         reward_step += 1000\n",
    "    #         done = True                             \n",
    "\n",
    "    #     observations = self.observations()                  # calculate observations\n",
    "        \n",
    "    #     if self.time_elapsed == self.time_limit:            # time-limit termination condition\n",
    "    #         done = True\n",
    "    #     else:\n",
    "    #         done = False\n",
    "    #         self.time_elapsed += 1              # update time\n",
    "    #         reward_step -= 1                    # negative reward per time-step\n",
    "        \n",
    "    #     self.render()                       # update visualisation\n",
    "    #     return observations, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_n = np.asarray([t[0] for t in self.P[self.agent_state][action]])\n",
    "# csprob_n = np.cumsum(prob_n)\n",
    "# i = (csprob_n > np_random.rand()).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.R = self.fill_reward_matrix()\n",
    "# self.P = self.fill_transition_matrix()    \n",
    "    \n",
    "    # def fill_transition_matrix(self):\n",
    "    #     state_transition_matrix = []\n",
    "    #     for i in range(len(self.land)):\n",
    "    #         for j in range(len(self.land)):\n",
    "    #             if i-1 < 0:\n",
    "    #                 state_up = None\n",
    "    #             else: state_up = np.array([i-1, j])\n",
    "    #             if i+1 > len(self.land)-1:\n",
    "    #                 state_down = None\n",
    "    #             else: state_down = np.array([i+1, j])\n",
    "    #             if j-1 < 0:\n",
    "    #                 state_left = None\n",
    "    #             else: state_left = np.array([i, j-1])\n",
    "    #             if j+1 > len(self.land)-1:\n",
    "    #                 state_right = None\n",
    "    #             else: state_right = np.array([i, j+1])\n",
    "    #             state_transition_matrix.append([state_up, state_down, state_left, state_right])\n",
    "    #     state_transition_array = np.array(state_transition_matrix, dtype=object).reshape(self.size*self.size, 4) # used to be 1 at the end\n",
    "    #     P = np.vsplit(state_transition_array, self.size) # P maps the position of the agent (the state) and action to reachable states - N*N is number of reachable states, 4 is number of possible actions \n",
    "    #     return P\n",
    "        \n",
    "    # def fill_reward_matrix(self): # change the values to reward values?\n",
    "    #     reward_matrix = []\n",
    "    #     for i in range(len(self.land)):\n",
    "    #         for j in range(len(self.land)):\n",
    "    #             if i-1 < 0:\n",
    "    #                 reward_up = None\n",
    "    #             else: reward_up = self.land[i-1][j]\n",
    "    #             if i+1 > len(self.land)-1:\n",
    "    #                 reward_down = None\n",
    "    #             else: reward_down = self.land[i+1][j]\n",
    "    #             if j-1 < 0:\n",
    "    #                 reward_left = None\n",
    "    #             else: reward_left = self.land[i][j-1]\n",
    "    #             if j+1 > len(self.land)-1:\n",
    "    #                 reward_right = None\n",
    "    #             else: reward_right = self.land[i][j+1]\n",
    "    #             reward_matrix.append([reward_up, reward_down, reward_left, reward_right])\n",
    "    #     reward_array = np.array(reward_matrix).reshape(self.size*self.size, 4) \n",
    "    #     R = np.vsplit(reward_array, self.size) # R maps the position of the agent (the state) and action to rewards - currently displays the obstacles but can be changed to rewaerd values\n",
    "    #     return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def fill_transition_probability_matrix(self):\n",
    "        state_transition_matrix = []\n",
    "        for i in range(len(self.land)):\n",
    "            for j in range(len(self.land)):\n",
    "                if i-1 < 0:\n",
    "                    state_up = None\n",
    "                elif self.land[i][j] == 0: \n",
    "                    state_up = np.array([[i-1, j], 1.0])\n",
    "                elif self.land[i][j] == 1: \n",
    "                    state_up = np.array([[i-1, j], 1.0])\n",
    "                elif self.land[i][j] == 3: \n",
    "                    state_up = np.array([[i-1, j], 1.0])\n",
    "                elif self.land[i][j] == 4: \n",
    "                    state_up = np.array([[i-1, j], 1.0])\n",
    "                elif self.land[i][j] == 5: \n",
    "                    state_up = np.array([[i-1, j], 1.0])\n",
    "                \n",
    "                if i+1 > len(self.land)-1:\n",
    "                    state_down = None\n",
    "                elif self.land[i][j] == 0: \n",
    "                    state_down = np.array([[i+1, j], 1.0])\n",
    "                elif self.land[i][j] == 1: \n",
    "                    state_down = np.array([[i+1, j], 1.0])\n",
    "                elif self.land[i][j] == 3: \n",
    "                    state_down = np.array([[i+1, j], 1.0])\n",
    "                elif self.land[i][j] == 4: \n",
    "                    state_down = np.array([[i+1, j], 1.0])\n",
    "                elif self.land[i][j] == 5: \n",
    "                    state_down = np.array([[i+1, j], 1.0])\n",
    "                \n",
    "                if j-1 < 0:\n",
    "                    state_left = None\n",
    "                elif self.land[i][j] == 0: \n",
    "                    state_left = np.array([[i, j-1], 1.0])\n",
    "                elif self.land[i][j] == 1: \n",
    "                    state_left = np.array([[i, j-1], 1.0])\n",
    "                elif self.land[i][j] == 3: \n",
    "                    state_left = np.array([[i, j-1], 1.0])\n",
    "                elif self.land[i][j] == 4: \n",
    "                    state_left = np.array([[i, j-1], 1.0])\n",
    "                elif self.land[i][j] == 5: \n",
    "                    state_left = np.asarray([[i, j-1], 1.0])\n",
    "            \n",
    "                if j+1 > len(self.land)-1:\n",
    "                    state_right = None\n",
    "                elif self.land[i][j] == 0: \n",
    "                    state_right = np.array([[i, j+1], 1.0])\n",
    "                elif self.land[i][j] == 1: \n",
    "                    state_right = np.array([[i, j+1], 1.0])\n",
    "                elif self.land[i][j] == 3: \n",
    "                    state_right = np.array([[i, j+1], 1.0])\n",
    "                elif self.land[i][j] == 4: \n",
    "                    state_right = np.array([[i, j+1], 1.0])\n",
    "                elif self.land[i][j] == 5: \n",
    "                    state_right = np.array([[i, j+1], 1.0])\n",
    "                state_transition_matrix.append([state_up, state_down, state_left, state_right])\n",
    "        state_transition_array = np.array(state_transition_matrix).reshape(self.size*self.size, 4) # used to be 1 at the end\n",
    "        det_P = np.vsplit(state_transition_array, self.size) # deterministic version of P"
   ]
  }
 ]
}