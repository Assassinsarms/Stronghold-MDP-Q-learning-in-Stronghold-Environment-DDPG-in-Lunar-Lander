{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM707 Coursework Task 2\r\n",
    "### Aaron Mir (Student Number: 160001207)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm implemented for the Stronghold environment to find the best policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\r\n",
    "## Stronghold\r\n",
    "\r\n",
    "The robot agent is preparing for an upcoming mission in which they are to infiltrate an enemy stronghold to gather intelligence on a potential coup d'Ã©tat. The enemy land is in the form of an NxN grid with each grid containing one stronghold of size N x N/2 (starting on either side of the middle column (randomly decided)) and a wide river surrounding the entire enemy land. The entire land is very misty, therefore the robot may not step where it is supposed to. Each move has a transition probability associated with it (0.7 to move to the chosen state or 0.3 to move to one of the other sides) (stochasticity). The stronghold has as many entrances from the mainland as (1/4)N and contains (1/5)N dangerous enemy combatants (at random positions) within the stronghold that move around randomly per step (stochasticity).  The amount of shore that has traps on it is (1/4)N x rows of shore. The column immediately beside the stronghold is normal land to prevent entrance blocking. The goal of the agent is to infiltrate the stronghold and gather the intelligence without being killed by enemy combatants or booby traps as fast as possible.\r\n",
    "\r\n",
    "This is a type of gridworld environment. The size (number of states) can be controlled by adjusting the grid dimensions.\r\n",
    "\r\n",
    "The environment is defined as follows:\r\n",
    "\r\n",
    "- The environment is a rectangular grid of states/cells. There are five different types of cells as indicated by the following cell labels: \r\n",
    "\r\n",
    "    - _ labels cells that are safe to step on i.e. normal land [0]\r\n",
    "\r\n",
    "    - X labels the cells that are walls i.e. river or wall and if the agent enters a wall cell, there is a penalty of -1 [1]\r\n",
    "    \r\n",
    "    - A labels the cell the agent is on, starts on a random cell on the shore [2]\r\n",
    "    \r\n",
    "    - T labels booby-trap cells and if the agent enters a booby-trap cell there is a pentalty of -1000 and the episode ends [3]\r\n",
    "\r\n",
    "    - E labels enemy cells and if the agent enters an enemy cell there is a pentalty of -1000 and the episode ends [4]\r\n",
    "\r\n",
    "    - I labels the intelligence cell and when reached gives a reward of 1000 and the episode ends [5]\r\n",
    "\r\n",
    "- There are four possible actions (Up, Down, Left, Right). \r\n",
    "\r\n",
    "- The transition function moves the agent in the expected direction with 0.7 probability, and there is a 0.3 probability of transitioning to one of the sides.\r\n",
    "\r\n",
    "- There is a reward of -1 for each action taken by the agent, which is intended to encourage the agent to reach the goal as fast as possible. If the agent runs out of time, the episode ends. \r\n",
    "\r\n",
    "- Episodes end whenever the agent falls in a booby-trap, gets killed by an enemy, reaches the goal/intelligence or runs out of time. The end-of-episode is modelled by transitioning to a zero-reward terminal state (all actions lead to that state). \r\n",
    "\r\n",
    "State of the agent: Governed by the state number of the cell it is on.\r\n",
    "\r\n",
    "States of the environment: Governed by the state number of the agent and the state number of moving enemies.\r\n",
    "\r\n",
    "Number of states of the environment is given by the size of the grid and the size of the stronghold in which enemies can move. e.g. for a 10x10 grid with a stronghold size of 6x3 and 6 enemies, the number of states is 100 - 6x3-1 +  (6x3-1 (because of starting state)/6)  - 1 (because of the intelligence in the stronghold)\r\n",
    "\r\n",
    "Rewards/Penalties: +1000 for getting intelligence, -1000 for getting hurt by a combatant or booby trap, -1 for moving into a wall or water, -1 per transition because of fuel constraints\r\n",
    "\r\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning works by building a table of action and states and learning the 'q-values' at each location. The values in the table are randomly (or zero) initialized at first and are learnt through the bellman equation.\r\n",
    "The bellman equation is \r\n",
    "\r\n",
    "<img src=\"Bellman Eq.png\" alt=\"Bellman\" width=\"1000\"/>\r\n",
    "\r\n",
    "The learning starts by first focussing more on the exploration and then on the exploitation. This is because in the start we don't know which is the optimal step and hence we must try to explore entire space and later do exploitation. Exploration simply means randomly selecting an action and exploitation means selecting the action with highest reward. This transition can be easily modelled using exponentially or linearly scheduling the exploration probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stronghold.stronghold import Stronghold\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist:\r\n",
    "\r\n",
    "    1. Build a Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 360x360 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stronghold = Stronghold(10)\r\n",
    "stronghold.reset()\r\n",
    "stronghold.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent policies the agent can take and evaluation \r\n",
    "class Policies():\r\n",
    "    def __init__(self, epsilon, lr, episodes, env_size, test_len = 100, action_space = 4, beta = 0.999, testing = True):\r\n",
    "        self.epsilon = epsilon\r\n",
    "        self.lr = lr\r\n",
    "        self.test_interval = test_interval\r\n",
    "        self.beta = beta\r\n",
    "        self.action_space = action_space\r\n",
    "        self.state_space = env_size * env_size + 1\r\n",
    "        self.q_table = np.zeros((self.state_space, self.action_space))\r\n",
    "        self.testing = testing\r\n",
    "\r\n",
    "    def explore_exploit_policy(self, current_state):\r\n",
    "        if np.random.uniform(0, 1) > self.epsilon:\r\n",
    "            return self.greedy_policy(current_state)\r\n",
    "        else:\r\n",
    "            return np.random.randint(0, self.action_space - 1)\r\n",
    "\r\n",
    "    def greedy_policy(self, current_state):\r\n",
    "        return np.argmax(self.q_table[curr_state,:])\r\n",
    "\r\n",
    "    def learn(self):\r\n",
    "        pass \r\n",
    "\r\n",
    "    def evaluate_policy(self, policy, trials = 100):\r\n",
    "        total_evaluation_reward = 0\r\n",
    "        for i in range(trials):\r\n",
    "            stronghold.reset()\r\n",
    "            steps = 0 \r\n",
    "            done = False\r\n",
    "            state, reward, done, info = stronghold.step(policy)\r\n",
    "            total_evaluation_reward += (self.beta ** steps) * reward\r\n",
    "            trial_reward = 0\r\n",
    "            steps += 1\r\n",
    "            while not done:\r\n",
    "                state, reward, done, info = stronghold.step(policy)\r\n",
    "                trial_reward += (self.beta ** steps) * reward\r\n",
    "                total_evaluation_reward += (self.beta ** steps) * reward\r\n",
    "                steps += 1\r\n",
    "                if view == True:\r\n",
    "                    print(\"state\", state)\r\n",
    "                    env.render()\r\n",
    "                    time.sleep(0.5)\r\n",
    "                    clear_output(wait=True)\r\n",
    "            print(\"Total reward for trial = \", trial_reward)\r\n",
    "        return total_evaluation_reward / trials\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Policies):\r\n",
    "    def __init__(self, epsilon, lr, episodes, env_size, test_len = 100, action_space = 4, beta = 0.999, testing = True):\r\n",
    "        super().__init__(epsilon, lr, episodes, env_size, test_len = test_len, action_space = action_space, beta = beta, testing = testing)\r\n",
    "        self.step_max = 100\r\n",
    "    \r\n",
    "    def Qlearn(self):\r\n",
    "        for i in range(self.test_interval):\r\n",
    "            current_state = stronghold.reset()\r\n",
    "            for j in range(self.step_max):\r\n",
    "                choice_action = self.explore_exploit_policy(current_state)\r\n",
    "                state, reward, done, info = stronghold.step(choice_action)\r\n",
    "                new_choice =  self.greedy_policy(state)\r\n",
    "\r\n",
    "                self.q_table[current_state, choice_action] = self.q_table[current_state, choice_action] + self.lr * \\\r\n",
    "                    (reward + self.beta *  self.q_table[state, new_choice] - self.q_table[current_state, choice_action])\r\n",
    "\r\n",
    "                current_state = new_state\r\n",
    "                if done == True:\r\n",
    "                    break\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stronghold.reset()\r\n",
    "epsilon = 0.3\r\n",
    "lr = 0.001\r\n",
    "episodes = 5000\r\n",
    "testing = False\r\n",
    "# do q learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "name": "python387jvsc74a57bd02db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}