{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM707 Coursework Task 3\r\n",
    "### Aaron Mir (Student Number: 160001207)\r\n",
    "## Soft Actor-Critic Implementation on Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\r\n",
    "import numpy as np\r\n",
    "import sys\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.autograd import Variable\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\r\n",
    "## Lunar Lander\r\n",
    "\r\n",
    "The environment is defined as follows:\r\n",
    "\r\n",
    "- The goal is to land the lunar module on the moon between the flags without crashing. The state vector is eight dimensional and contains: [x_pos, y_pos, x_vel, y_vel, lander_angle, lander_angular_vel, left_leg_\r\n",
    "ground_contact_flag, right_leg_ground_contact_flag]. \r\n",
    "\r\n",
    "- The landing pad coordinates are always at 0, 0 and are the first two numbers in the state vector.\r\n",
    "\r\n",
    "- There are two possible actions which take float values [main engine, left-right engines].\r\n",
    "\r\n",
    "    - Main engine: from -1 to 0, the engine is off and between 0 and +1, the engine throttle is from 50% to 100% power. The engine can't work with less than 50% power.\r\n",
    "\r\n",
    "    - Left-right engines: from -1 to -0.5, the left engine is on. From +0.5 to +1, the right engine is on. Between -0.5 and 0.5, both engines are off.\r\n",
    "    \r\n",
    "- The agent recieves +100-140 reward for moving from the top of the screen to the landing pad, a -100 penalty for crashing into the landing pad and an additional +100 reward for landing on the pad at rest. Each leg that is in contact with the ground is +10. Firing the main engine is -0.3 penalty for each frame. \r\n",
    "\r\n",
    "- The environment is considered solved when the agent gets +200 reward.\r\n",
    "\r\n",
    "- Episodes end whenever the agent crashes or comes to a rest.\r\n",
    "\r\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(ActorCritic, self).__init__()\r\n",
    "        self.fc1 = nn.Linear(states_in, 128)\r\n",
    "        self.actor = nn.Linear(128, num_actions_in)\r\n",
    "        self.critic = nn.Linear(128, 1)\r\n",
    "\r\n",
    "    def forward(self, state):\r\n",
    "        x = F.relu(self.fc1(state))\r\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "name": "python387jvsc74a57bd02db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}